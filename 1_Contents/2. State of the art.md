##  2.1 Vehicle Routing Problems

###  2.1.1 The Conventional Vehicle Routing Problem

The most fundamental and well-studied routing problem is without doubt the Traveling Salesman Problem (TSP) , in which a salesman is to visit a set of cities and return to the city he started in. The objective for the TSP is to minimize the total distance traveled by the salesman.
-> Reference TSP first paper

---
The CVRP is first described by Dantzig and Ramser [dantzig1959] as follows: 

The Vehicle Routing Problem (VRP) is a generalization of the TSP in that the VRP consists in determining m vehicle routes, where a route is a tour that begins at the depot, visits a subset of the customers in a given order and returns to the depot. All customers must be visited exactly once and the total customer demand of a route must not exceed the vehicle capacity. The objective of the VRP is to minimize the overall distribution costs. In most real-life distribution contexts a number of side constraints complicate the model.

A number of identical vehicles with a given capacity are located at a central depot. They are available for servicing a set of customer orders, (all deliveries, or, alternatively, all pickups). Each customer order has a specific location and size. Travel costs between all locations are given. The goal is to design a least cost set of routes for the vehicles in such a way that all customers are visited once and vehicle capacities are adhered to.

<font color="#ff0000">-> Graph Representation (picture)</font>


Laporte et al. (1985) proposed a mixed integer linear programming formulation for the CVRP, where the integer variable xe indicates the number of times that edge e ∈ A is traversed in the solution. Let δ(S) = {(i, j) : i ∈ S, j /∈ S or i /∈ S, j ∈ S} for S ⊆ N . Let v(S) denote the minimum number of vehicles needed to serve the node set S, which can be obtained by solving the Bin Packing Problem (BPP) with S and bins of capacity q. The formulation is as follows:


The degree constraints (2.2) and (2.3) make sure that each customer is visited exactly once and m routes are created. Constraints (2.4) impose both the connectivity of the solution and vehicle capacity requirement by forcing a sufficient number of edges to enter each subset of nodes. Constraints (2.5) ensure that each edge between two customers is traversed at most once. Constraints (2.6) state that each edge between the depot and customer can be traversed at most twice. If a vehicle performs a single-customer route, the edge between the depot and the customer will be traversed twice.

Another useful formulation of the CVRP is the set partitioning formulation, in which all the feasible routes are enumerated and their costs are determined in advance (Bramel and Simchilevi (2002)). The objective is to find a set of routes that form a feasible solution with minimum total cost. Let R denote the set of all feasible routes, each of which has a load less than or equal to the vehicle capacity. The cost of each route r ∈ R is denoted by cr. Note that cr is the minimum cost of serving the customers covered by r. Parameter ari states
whether customer i is covered by route r. Binary variable zr equals 1 if route r is selected and 0 otherwise. The CVRP is then formulated as follows:

###  Other VRP Variants

Static Vehicle Routing Problem. 1. All information relevant to the planning of the routes is assumed to be known by the planner before the routing process begins. 2. Information relevant to the routing does not change after the routes have been constructed.

Dynamic Vehicle Routing Problem. 1. Not all information relevant to the planning of the routes is known by the planner when the routing process begins. 2. Information can change after the initial routes have been constructed.

Probabilistic VRP/Stochastic VRP







### 2.1.2 Exact Algorithmic Solution Methods
branch-and-price algorithm (Christiansen and Lysgaard 2007)
branch-and-cut algorithms (Augerat et al. 1995; Ralphs et al. 2003; Baldacci, Hadjiconstantinou, and Mingozzi 2004)
### 2.1.3 Heuristic Solution Methods

constructive heuristics

improvement heuristics

metaheuristics (Laporte and Semet (2002)).
--> single-point based heuristics:
	simulated annealing (SA) (Kirkpatrick, Gelatt, and Vecchi 1983)
	tabu search (TS) (Glover and Laguna 1997)
	GRASP (for greedy 6 randomized adaptive search procedure) (Feo and Resende 1995)
	variable neighborhood search (VNS) (Mladenovic and Hansen 1997)
	guided local search (GLS) (Voudouris and Tsang 2003)
	iterated Local Search (ILS) (Stützle 1999)
	large neighborhood search (LNS) 
	adaptive large neighborhood search (ALNS) Pisinger and Ropke (2010)
	
--> population-based heuristics (inspired by some natural phenomena):
	evolutionary algorithms
	swarm intelligence, which

## 2.2 Machine Learning
Reinforcement Learning (RL)
Deep Reinforcement Learning (DRL)
Supervised Learning (SL) -> autoregressive (AR) and non-autoregressive (NAR) categories
Markov Decision Process (MDP)
Q-Learning
Graph Neural Networks (GNN)
Decision Trees

Machine Learning (ML) provides a set of data-driven methods that enable computers to **learn** and improve their performance on tasks without being explicitly programmed. ML algorithms are generally grouped into three broad paradigms: **supervised learning**, **unsupervised learning**, and **reinforcement learning**​

[researchgate.net](https://www.researchgate.net/figure/Basic-structures-of-the-three-learning-paradigms-supervised-learning-reinforcement_fig1_260652455#:~:text=,)

. Each paradigm defines a different learning setting based on the type of feedback or data available to the learning algorithm. In this section, we introduce these paradigms and then discuss core ML techniques and models relevant to later VRP applications. The aim is to give a high-level understanding of key concepts such as neural networks, support vector machines, clustering, decision trees, Markov decision processes, Q-learning, and graph neural networks. Throughout, we keep explanations intuitive and **non-mathematical**, providing examples and noting relevance to VRP when appropriate (without yet diving into VRP-specific details).

## ML Learning Paradigms

### Supervised Learning (SL)

In **supervised learning**, the algorithm learns from example input-output pairs provided by a _labeled_ dataset. Each training sample comes with an input (often a feature vector representing a data point) and an associated **target output** or label. The goal is for the model to **learn** a mapping from inputs to outputs such that it can correctly predict the outputs for new, unseen inputs​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=Reinforcement%20Learning%20,error%20exploration.%20RL%20revolves%20around)

. In other words, the model generalizes from the given examples to infer the underlying function that produces the correct output from any input. Supervised learning is widely used for **classification** (predicting discrete labels, e.g. determining which category an item belongs to) and **regression** (predicting continuous values) tasks. Classic examples include image classification (input: image, output: object category) and demand forecasting (input: historical data, output: future value). The learning process in SL typically involves minimizing a loss function that quantifies the error between the model’s predictions and the true labels in the training data.

**Autoregressive (AR) vs. Non-Autoregressive (NAR) Models:** Some supervised tasks involve predicting a _sequence_ or structured output (for instance, predicting an entire route in a VRP or generating a sentence word-by-word). In such cases, one can distinguish between autoregressive and non-autoregressive approaches. An **autoregressive model** generates the output sequence **step-by-step**, each step conditioned on the previous outputs. In other words, the model produces one element of the sequence at a time (e.g. one city in a route or one word in a sentence), feeding that output back into the model to help generate the next element. This typically yields high accuracy because the model captures dependencies between output elements, but it is inherently sequential and can be slow at inference​

[arxiv.org](https://arxiv.org/abs/2004.10454#:~:text=but%20not%20all%3F%20,tasks%2C%20ASR%20has%20the%20most)

. By contrast, a **non-autoregressive model** attempts to generate all (or multiple) parts of the output in parallel without conditioning on its own earlier outputs​

[aclanthology.org](https://aclanthology.org/2022.acl-tutorials.4/#:~:text=)

. NAR models sacrifice some dependency modeling in exchange for much faster generation. For example, in neural machine translation and some ML-based VRP solvers, NAR models can output an entire route or translation almost simultaneously, whereas AR models output one step at a time. The trade-off is that NAR approaches may achieve slightly lower accuracy or require more complex methods to model sequence structure, whereas AR approaches, though slower, often attain higher fidelity by accounting for the sequential dependencies​

[arxiv.org](https://arxiv.org/abs/2004.10454#:~:text=but%20not%20all%3F%20,tasks%2C%20ASR%20has%20the%20most)

​

[aclanthology.org](https://aclanthology.org/2022.acl-tutorials.4/#:~:text=Non,In%20this)

. In summary, AR vs. NAR is an important design consideration for sequence prediction problems: AR models sequentially incorporate feedback yielding thorough dependency modeling, while NAR models prioritize speed by forgoing sequential feedback.

### Unsupervised Learning (UL)

In **unsupervised learning**, the data consists of _unlabeled_ examples – there are inputs but no explicit target outputs. The goal in UL is to discover **patterns, structures, or relationships** in the data without any ground-truth reference. In other words, the learning algorithm tries to organize or describe the data in some way, purely based on the inherent characteristics of that data​

[citeseerx.ist.psu.edu](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8ff7b7cf3849640b7cfb8f08e2946fd151fed34c#:~:text=Clustering%20is%20the%20unsupervised%20classification,has%20made%20the%20transfer%20of)

. Common unsupervised tasks include **clustering**, **dimensionality reduction**, and **anomaly detection**. In **clustering**, for instance, the algorithm groups data points into clusters such that points in the same cluster are more similar to each other (in some defined sense) than to those in other clusters​

[citeseerx.ist.psu.edu](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8ff7b7cf3849640b7cfb8f08e2946fd151fed34c#:~:text=Clustering%20is%20the%20unsupervised%20classification,has%20made%20the%20transfer%20of)

. A classic clustering algorithm is _K-means_, which partitions the dataset into _K_ clusters by iteratively assigning points to the nearest cluster centroid and then updating the centroids. Another example is _DBSCAN_ (Density-Based Spatial Clustering of Applications with Noise), a clustering method that can find clusters of arbitrary shape by growing regions of high point density​

[medium.com](https://medium.com/@tarammullin/dbscan-2788cfce9389#:~:text=DBSCAN%20%E2%80%94%20Overview%2C%20Example%2C%20%26,1996)

. Unsupervised learning is often used in exploratory data analysis – for example, discovering customer segments from sales data without prior labels, or reducing high-dimensional data (with methods like PCA) to visualize it. In the context of VRP, unsupervised techniques might be used to identify patterns in customer locations or demand profiles without a predetermined “right answer.” The key feature of UL is that the **learning signal comes from the data itself**, by leveraging the data’s internal structure (as opposed to a provided label or reward). Because there are no labels, evaluation of unsupervised learning results can be subjective, relying on measures of cluster quality or the usefulness of learned representations for downstream tasks​

[citeseerx.ist.psu.edu](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8ff7b7cf3849640b7cfb8f08e2946fd151fed34c#:~:text=broad%20appeal%20and%20usefulness%20as,and%20references%20to%20fundamental%20concepts)

.

### Reinforcement Learning (RL and DRL)

**Reinforcement learning** differs significantly from SL and UL. In RL, an **agent** learns to make a sequence of decisions by interacting with an **environment**, observing the state of the environment, and taking actions; after each action, the agent receives a **reward** signal that indicates the immediate utility or feedback for that action. The agent’s objective is to learn a **policy** (a strategy mapping states to actions) that maximizes the cumulative reward it receives over time​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=Reinforcement%20Learning%20,error%20exploration.%20RL%20revolves%20around)

. This paradigm is inspired by how animals learn through trial-and-error: the agent is not told the correct action to take (as in supervised learning) but must discover actions that yield high reward through experimentation. Over time, actions leading to higher rewards should be taken more frequently. RL is often formalized through the framework of a **Markov Decision Process (MDP)**, which provides a mathematical model of sequential decision-making. An MDP consists of a set of states describing the environment, a set of possible actions the agent can take in each state, transition probabilities that govern how actions cause the environment to change state, and a reward function that assigns rewards for state transitions​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=interventions%20requires%20the%20estimation%20of,of%20MDPs%20being%20used%20in)

​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=and%20%2C%20derive%20equations%20for,which%20is%20defined%20as%20follows)

. The _Markov_ property implies that the future state depends only on the current state and action, not on the history of past states​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=and%20%2C%20derive%20equations%20for,which%20is%20defined%20as%20follows)

. The agent’s goal is to learn an optimal policy – often denoted π* – that maximizes the expected **return**, which is the cumulative sum of (possibly discounted) rewards over time.

A core challenge in RL is the **credit assignment problem**: how to attribute which actions were responsible for eventually obtaining rewards, especially when rewards may be delayed. Many RL algorithms address this by learning a **value function** that estimates how good each state (or state-action pair) is in terms of future rewards. One foundational algorithm is **Q-learning** (proposed by Watkins & Dayan in 1992), which is a model-free method that learns an action-value function Q(s,a). This Q-value represents the expected cumulative reward an agent will get if it starts from state _s_, takes action _a_, and then follows the optimal policy thereafter​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=to%20proceed%20%28known%20as%20bootstrapping%29,a%20given%20action%20in%20each)

. By iteratively updating Q-values based on experience (using the Bellman equation as an update rule), the agent improves its estimates of which actions are favorable. Notably, Q-learning is **off-policy** and can converge to the optimal Q-values (and thus an optimal policy) given sufficient exploration of the state space​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=to%20proceed%20%28known%20as%20bootstrapping%29,a%20given%20action%20in%20each)

. Another class of RL algorithms learns policies directly (e.g. _policy gradient_ methods), but those are beyond our current scope.

When RL algorithms are combined with **deep neural networks** as function approximators, we speak of **Deep Reinforcement Learning (DRL)**. In Deep RL, a neural network can represent the policy or value function, enabling the agent to handle very large or continuous state spaces by learning complex representations. A landmark example is the Deep Q-Network (DQN) introduced by _Mnih et al._ (2015), which used a convolutional neural network to estimate Q-values from raw pixel inputs of Atari video games​

[pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/25719670/#:~:text=or%20to%20domains%20with%20fully,the%20same%20algorithm%2C%20network%20architecture)

. This approach famously achieved human-level performance on many Atari games, with the agent receiving only game screens and game scores as input and learning via RL to maximize its score​

[pubmed.ncbi.nlm.nih.gov](https://pubmed.ncbi.nlm.nih.gov/25719670/#:~:text=or%20to%20domains%20with%20fully,the%20same%20algorithm%2C%20network%20architecture)

. The success of DQN and related algorithms demonstrated the power of DRL, combining reinforcement learning’s trial-and-error search with deep learning’s representation learning. In the context of VRP, one can conceptualize an RL agent that incrementally constructs routes (actions) and receives a reward based on route quality (e.g., negative cost), thereby learning to build better routes over time. Indeed, several recent works use RL or DRL to tackle VRPs, treating the route construction as a sequential decision process. We will explore those in the literature review, but the key RL concepts needed are the idea of an agent optimizing cumulative reward, the MDP framework, and algorithms like Q-learning that underpin many such approaches.

## Core Machine Learning Techniques and Models

Having outlined the main learning paradigms, we now introduce key ML techniques and models that will appear in later chapters. These include various types of neural networks (which are prominent in modern ML, especially for DRL-based VRP methods), as well as important classical methods like SVMs, decision trees, and clustering algorithms. We also describe graph neural networks, a modern deep learning architecture particularly relevant for problems set on graphs (such as VRPs).

### Neural Networks and Deep Learning

**Artificial Neural Networks (ANNs)** – often just called _neural networks_ – are a family of models inspired by the brain’s interconnected neurons. At a high level, a neural network is composed of layers of simple computational units (_neurons_ or _nodes_) that apply weighted sums and nonlinear activation functions to their inputs. The neurons are arranged in layers (an **input layer**, one or more **hidden layers**, and an **output layer**), and the output of each layer serves as input to the next. Through a process of training (typically using gradient-based optimization like backpropagation), the network adjusts the weights on connections between neurons to fit the data. Neural networks are extremely flexible function approximators; with enough hidden units, a network can approximate complex nonlinear mappings from inputs to outputs [Goodfellow et al., 2016]. This expressiveness, along with advances in algorithms and compute, has led to neural networks (especially _deep_ neural networks with many layers) becoming central in modern ML.

There are several important neural network **architectures** to know:

- **Multi-Layer Perceptron (MLP):** This is the basic form of a feed-forward neural network, consisting of fully-connected layers of neurons. Each neuron in a layer takes a weighted sum of all outputs from the previous layer and passes it through a nonlinear activation (such as ReLU or sigmoid). MLPs (also known simply as dense feed-forward networks) are general-purpose and can be used for a variety of tasks (classification, regression) on structured data. They do not assume any particular structure in the input (unlike CNNs for images or RNNs for sequences), so they treat the input as a simple feature vector. MLPs were among the earliest neural nets and have been applied to problems like predicting travel times or estimating costs in routing problems, by learning from examples of inputs and outputs.
    
- **Convolutional Neural Network (CNN):** CNNs are neural networks designed to process data with a grid-like topology, such as images (2D grids of pixels) or time series (1D grids)​
    
    [cs.toronto.edu](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf#:~:text=the%20previous%20layer,such%20as%20text%20and%20speech)
    
    . Rather than full connections, CNN layers use _convolutional filters_ that connect each neuron only to a local region of the input (e.g. a patch of an image), and the same set of weights (filter) is reused across the entire input. This leverages spatial/local correlations and drastically reduces the number of parameters. CNNs often include pooling layers that down-sample features, providing some translational invariance. CNN architectures have been **hugely successful in image and vision tasks**, achieving breakthrough performance in image recognition and detection​
    
    [cs.toronto.edu](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf#:~:text=the%20previous%20layer,such%20as%20text%20and%20speech)
    
    . For example, in the 2012 ImageNet competition, a deep CNN famously halved the error rate relative to previous approaches​
    
    [cs.toronto.edu](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf#:~:text=the%20detection%2C%20segmentation%20and%20recognition,labelled%20data%20was%20relatively%20abun%02dant)
    
    . In VRP contexts, CNNs might appear if we treat input data as images or grids (for instance, representing a city map or a demand distribution as a grid), although they are more common in domains like computer vision. The key advantage of CNNs is their ability to extract hierarchical features (from edges and textures up to object parts in images) through stacked convolutional layers, making them very effective for spatial data​
    
    [cs.toronto.edu](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf#:~:text=the%20previous%20layer,such%20as%20text%20and%20speech)
    
    .
    
- **Recurrent Neural Network (RNN):** RNNs are neural networks tailored for **sequential data**. They introduce feedback connections, allowing information to persist from one step of the sequence to the next. In an RNN, the network processes one element of the sequence at a time (e.g. one word in a sentence, or one customer in a route sequence) and maintains a hidden state that carries information about previous elements. This enables the network to model temporal or sequential dependencies. Variants like **LSTM** (Long Short-Term Memory) and **GRU** (Gated Recurrent Unit) use gating mechanisms to better preserve long-range dependencies and mitigate issues like vanishing gradients. RNNs and their variants have excelled in tasks such as language modeling, translation, and speech recognition​
    
    [cs.toronto.edu](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf#:~:text=the%20previous%20layer,such%20as%20text%20and%20speech)
    
    , effectively handling sequences of varying length. In routing problems, one might use an RNN or related architecture to sequentially construct a route: the RNN hidden state can encode the route constructed so far, and at each step the network chooses the next location to visit. Indeed, some pioneering ML approaches to VRP used sequence-to-sequence models (with RNN encoders/decoders) to output routes. However, RNNs can be challenging to train on long sequences and have largely been supplemented or replaced in many applications by Transformer architectures (which use self-attention mechanisms), though those go beyond our current scope.
    

Neural networks trained with many layers (dozens or even hundreds) are referred to as **deep neural networks**, and the field of _deep learning_ studies how to effectively train such models. Deep learning has driven many recent improvements in AI, in part because these models can automatically learn rich hierarchical representations of data. For example, as described by LeCun, Bengio and Hinton (2015), higher layers in a deep network learn increasingly abstract features of the input – in an image, early layers detect edges, mid-layers detect shapes or motifs, and top layers recognize objects​

[cs.toronto.edu](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf#:~:text=Representation%20learning%20is%20a%20set,tasks%2C%20higher%20layers%20of%20representation)

​

[cs.toronto.edu](https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf#:~:text=Deep%20learning%20is%20making%20major,analysing%20particle%20accelerator%20data9%2C10)

. The key point is that neural networks are powerful **learning machines** that can be adapted (via architecture and training) to many problem domains. Later in our VRP literature review, we will encounter methods that use neural networks to construct routes, evaluate routes, or even directly output optimized routes. An understanding of MLPs, CNNs, and RNNs provides a foundation for grasping those methods. (For a more extensive introduction to neural networks and deep learning, see texts such as _Goodfellow et al. (2016)_ or _Bishop (2006)_.)

### Support Vector Machines (SVM)

Support Vector Machines are a well-established class of supervised learning models, primarily used for classification (and also regression in the form of Support Vector Regression). The core idea of an SVM is to find the **optimal separating hyperplane** that best divides the data into classes. In a simple binary classification setting (with two classes), an SVM searches for the hyperplane (a linear decision boundary in feature space) that maximizes the **margin** – the distance between the hyperplane and the nearest data points of each class​

[arxiv.org](https://arxiv.org/html/2410.09186v1#:~:text=image%20denoising%20and%20compression%2C%E2%80%9D%20IEEE,a)

. These closest points are the eponymous _support vectors_. By maximizing the margin, SVMs aim to improve generalization, on the intuition that a decision boundary that cleanly separates classes with a wide gap will perform well on new data.

For data that are not linearly separable in the original input space, SVMs use the **kernel trick**: the input features are implicitly mapped to a higher-dimensional space via a kernel function, in which a linear separator may exist. Common kernels include the polynomial kernel and the radial basis function (RBF) kernel. By choosing an appropriate kernel, SVMs can effectively perform nonlinear classification while still maintaining the efficiency of linear separation in the kernel-defined feature space. The theory of SVMs was developed in the 1990s by Cortes and Vapnik​

[arxiv.org](https://arxiv.org/html/2410.09186v1#:~:text=image%20denoising%20and%20compression%2C%E2%80%9D%20IEEE,a)

, and SVMs became a dominant method for many classification tasks in the late 1990s and 2000s due to their strong performance and well-founded theoretical guarantees. They were widely applied in areas such as text categorization, image recognition, and bioinformatics.

For the purposes of our discussion, SVMs represent a **classical ML approach** that might be contrasted with newer deep learning approaches. While modern VRP research often favors neural network or reinforcement learning models, one can still find instances where SVMs (or related linear models) could be used – for example, as components in hybrid systems or for simpler prediction tasks within a VRP (such as predicting travel time categories, or classifying problem instances). SVMs are known for working well with smaller to medium-sized datasets and for their ability to handle high-dimensional data. They also yield models that are fairly interpretable through support vectors and have convex optimization at their core (hence a unique global solution). In summary, SVMs are margin-based classifiers that find a decision boundary with maximal separation between classes, offering a robust and principled approach to supervised learning​

[arxiv.org](https://arxiv.org/html/2410.09186v1#:~:text=image%20denoising%20and%20compression%2C%E2%80%9D%20IEEE,a)

.

### Decision Trees

A **decision tree** is a tree-structured model used for both classification and regression tasks. It learns a set of if-then decision rules from data features that lead to a prediction. Each internal node of the tree corresponds to a decision based on the value of one of the input features, each branch represents an outcome of that decision (e.g. _feature j > 5_ goes left vs. _<= 5_ goes right), and each leaf node gives a final prediction (a class label or a numeric value). Decision trees are learned by recursively partitioning the data: at each step, the algorithm chooses a feature and a split that best separates the data into subsets that are purer in terms of the target label (for classification) or have lower variance (for regression). Criteria such as **information gain** (based on entropy) or **Gini impurity** are commonly used to select splits in classification trees (e.g., as in the ID3, C4.5 (Quinlan), or CART algorithms)​

[link.springer.com](https://link.springer.com/doi/10.1007/978-0-387-30164-8_204#:~:text=Decision%20trees%20are%20learned%20in,and)

.

One of the main advantages of decision trees is their **interpretability**. The flowchart-like structure is easy to understand – one can trace a path from the root to a leaf to see the logic the model uses for a prediction. For example, a simple decision tree for classifying whether a vehicle should take a certain route might split first on “Is the distance > X?”, then perhaps on “Is there a time window constraint satisfied?”, etc., leading to a yes/no decision at the leaves. Each path provides a set of conditions that yields a particular decision, which is human-interpretable.

On their own, single decision trees can be prone to overfitting (they can memorize the training data if grown too deep). However, when _ensemble_ methods are applied (like random forests, which average many trees, or boosted trees, which add trees sequentially to correct errors), tree-based models become extraordinarily powerful. Ensemble tree methods have been among the top-performing algorithms for structured data in many contexts. In the scope of this chapter, however, we consider a single decision tree as a fundamental concept. They represent the idea of _greedy, hierarchical partitioning_ of data based on feature tests. In VRP-related research, decision trees might be used in heuristic selection (learning rules for which heuristic to apply on a given VRP instance) or in approximating value functions (in some hybrid RL approaches), though by themselves they are not a common standalone solution for VRPs. Still, understanding decision trees is useful, as they are a basis for more complex models and often serve as a baseline due to their simplicity and interpretability.

### Clustering Methods (K-Means, DBSCAN)

Clustering, as introduced under unsupervised learning, is the task of grouping a set of data points into clusters such that points in the same cluster are more similar to each other than to those in other clusters​

[citeseerx.ist.psu.edu](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=8ff7b7cf3849640b7cfb8f08e2946fd151fed34c#:~:text=Clustering%20is%20the%20unsupervised%20classification,has%20made%20the%20transfer%20of)

. It’s worth highlighting two popular clustering techniques: **K-Means** and **DBSCAN**, as they exemplify two different approaches (centroid-based and density-based, respectively) and might be referenced in literature.

- **K-Means Clustering:** K-Means is a simple and widely used algorithm that requires the user to specify the number of clusters _K_. The algorithm then iteratively refines a set of _K_ cluster centroids. Initially, centroids can be chosen randomly. Then it alternates between two steps: (1) **Assignment step:** assign each data point to the nearest centroid (typically using Euclidean distance), forming _K_ clusters; (2) **Update step:** recompute each centroid as the mean of all data points assigned to that cluster. These steps repeat until convergence (no point changes cluster or centroids stabilize). K-Means tends to produce clusters that are roughly spherical (because it uses means and distance) and works well when clusters are compact and well-separated. It is efficient and easy to implement, though it can get stuck in local optima (so it’s common to run it multiple times with different initializations). In VRP contexts, one might use K-means to cluster customers based on location as a heuristic approach (e.g., treat each cluster as a “sub-problem” region), or to analyze geographic patterns in demand. However, K-means itself doesn’t incorporate information like road network distance – it’s purely based on point similarity in feature space.
    
- **DBSCAN (Density-Based Spatial Clustering of Applications with Noise):** DBSCAN is a clustering algorithm that does not require specifying the number of clusters in advance. Instead, it requires two parameters: a radius ε and a minimum number of points _MinPts_. DBSCAN works by identifying “dense” regions of points: it starts from an arbitrary point and finds all points within distance ε. If that set has at least _MinPts_ points, it forms a **core** of a cluster; then DBSCAN recursively expands the cluster by adding all points within ε of any point in the cluster. In effect, clusters grow as long as there are neighboring points within ε, and points that lie in sparse regions (fewer than _MinPts_ neighbors within ε) are labeled as noise/outliers. A major benefit of DBSCAN is that it can find clusters of arbitrary shape (not just spherical), and it automatically determines the number of clusters based on data density​
    
    [medium.com](https://medium.com/@tarammullin/dbscan-2788cfce9389#:~:text=DBSCAN%20%E2%80%94%20Overview%2C%20Example%2C%20%26,1996)
    
    . It’s effective for data where clusters are of varying size and shape, and for detecting outliers. In routing problems, one might imagine using a density-based approach to identify concentrated areas of deliveries (clusters of customers that are geographically dense) versus isolated customers (which DBSCAN would label noise) – which could then inform route planning by handling dense clusters separately from isolated points.
    

Clustering algorithms are fundamental in unsupervised learning and can also serve as a preprocessing step. For example, clustering customers in a VRP might reduce problem complexity or inspire a hierarchical routing strategy (first assign clusters to vehicles, then fine-tune routes within clusters). The key point is that clustering finds **structure** in data without labels: whether by seeking compact groups (K-means) or connected dense regions (DBSCAN) or other criteria (hierarchical clustering, Gaussian mixtures, etc.). Many other clustering methods exist, but these two provide a good illustration of the concept.

### Markov Decision Processes and Q-Learning (Recap)

_(This subsection reinforces some points from the RL section, given their importance as standalone concepts.)_

A **Markov Decision Process (MDP)** is the formal framework underlying most reinforcement learning problems. As defined earlier, an MDP is characterized by a set of states _S_, a set of actions _A_ available to an agent, transition probabilities P(s’ | s, a) describing the chance of moving to state _s’_ when action _a_ is taken in state _s_, and a reward function _R(s, a, s’)_that gives the immediate reward for transitioning from _s_ to _s’_ via action _a_​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=interventions%20requires%20the%20estimation%20of,of%20MDPs%20being%20used%20in)

. Additionally, there may be a discount factor γ ∈ [0,1] if we want to prioritize immediate rewards over future rewards. The Markov property implies that the probability of _s’_ (and the reward) depends only on the current state and action, not on how that state was reached​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=and%20%2C%20derive%20equations%20for,which%20is%20defined%20as%20follows)

. An MDP provides a **normative model** for sequential decision making: solving an MDP means finding a policy π that maximizes the expected return (cumulative reward). Dynamic programming methods (like value iteration and policy iteration) can solve MDPs if the model (P and R) is known and state spaces are small, but in many realistic problems (like VRPs cast as an MDP) the state space is huge and the model might be unknown – this is where reinforcement learning techniques come into play.

**Q-Learning** is one such technique: a model-free RL algorithm that learns the optimal _action-value function_, commonly denoted Q*(s,a). Q*(s,a) represents the maximum expected return achievable starting from state _s_, taking action _a_, and thereafter following an optimal policy. Q-learning iteratively approximates this function using the Bellman optimality equation:

Qnew(s,a)←Q(s,a)+α[r+γmax⁡a′Q(s′,a′)−Q(s,a)],Qnew​(s,a)←Q(s,a)+α[r+γmaxa′​Q(s′,a′)−Q(s,a)],

where _s’_ is the state resulting from action _a_ in _s_ and _r_ is the reward received​

[arxiv.org](https://arxiv.org/html/2408.07712#:~:text=to%20proceed%20%28known%20as%20bootstrapping%29,a%20given%20action%20in%20each)

. This update moves Q(s,a) toward the observed reward plus the estimated value of the best action in the next state (the term in brackets is the temporal-difference error). Over many episodes of interaction, Q-values converge to Q* under certain conditions (like sufficient exploration of actions in all states). One important aspect of Q-learning is that it is **off-policy**, meaning it learns about the optimal policy _while_ possibly following a different exploration policy (like ε-greedy, which sometimes picks random actions). Eventually, the learned Q-values implicitly define an optimal policy: in any state s, one can choose argmaxₐ Q(s,a).

Q-learning has been very influential because of its simplicity and guarantee of convergence. In practice, for complex problems, Q-values may be stored in a table for small state spaces, or approximated with function approximators (like neural networks in DQN, as discussed). In a VRP context, one could conceptualize Q-learning to learn which location to visit next given a current partial route (state) and a candidate next stop (action), with rewards defined to encourage efficient route completion. However, VRP state spaces are extremely large (combinatorial in route sequences), so applying basic Q-learning directly is infeasible – instead, advanced schemes (like deep RL with policy networks, or learning heuristics) are used. Nonetheless, understanding Q-learning provides a basis for grasping more advanced RL algorithms encountered in the VRP literature.

### Graph Neural Networks (GNN)

Many ML approaches assume data in Euclidean formats (like vectors, sequences, or images), but **graph neural networks**extend deep learning to data that are naturally represented as graphs. A graph is a set of **nodes** (vertices) connected by **edges**, and many problems (including VRP) are most naturally viewed in graph terms – for instance, in VRP we often have a graph where nodes represent customers or locations and edges represent roads or distances. **Graph Neural Networks (GNNs)** are neural models that directly operate on graph-structured data by **capturing dependencies among nodes via message passing between neighbors**​

[arxiv.org](https://arxiv.org/abs/1812.08434#:~:text=Graph%20neural%20networks%20,open%20problems%20for%20future%20research)

. In a typical GNN, each node starts with an initial feature vector (which could represent properties like demand or coordinates), and the network iteratively updates the node representations by aggregating information from their neighbors. This process is often called _message passing_: each node sends a "message" (a function of its current features) along its edges to neighboring nodes, and each node then updates its own feature by combining the messages received from neighbors (and possibly its old feature)​

[arxiv.org](https://arxiv.org/abs/1812.08434#:~:text=Graph%20neural%20networks%20,open%20problems%20for%20future%20research)

. After several rounds (layers) of message passing, the node features encode structural information from the graph within a certain neighborhood.

As a result, GNNs produce **embeddings** (vector representations) for nodes (and sometimes for entire graphs or edges) that reflect the graph’s connectivity and feature patterns. These embeddings can then be used for various tasks: for example, a GNN can be trained to output a score or label for each node (node classification), to predict a value for the whole graph (graph regression or classification), or to predict the existence of edges (link prediction). GNNs have shown remarkable success in many applications, from social network analysis to chemistry (predicting molecular properties where atoms are nodes)​

[journalofbigdata.springeropen.com](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00876-4#:~:text=Graph%20Neural%20Networks%20,This%20information%20often%20manifests%20as)

​

[journalofbigdata.springeropen.com](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00876-4#:~:text=A%20graph%20is%20a%20data,This%20makes%20applying)

. In combinatorial optimization and routing, GNNs are very promising because they can learn heuristics that generalize across different graph-structured problem instances (e.g., different road networks or customer distributions) by exploiting the relational structure. For instance, a GNN could be used to encode a VRP instance (locations as nodes, distances as weighted edges) and then a separate network component might decode a route solution from those embeddings. The GNN’s node embeddings would capture information like which locations are “important” or which are near each other, which can guide the construction of routes.

One key feature of GNNs is that they are typically **inductive** – the learned message-passing function can be applied to graphs of different sizes and shapes, not just the ones seen in training. This is valuable for VRP, as real-world problem instances vary in number of customers and graph topology. Popular types of GNNs include _Graph Convolutional Networks (GCN)_, _Graph Attention Networks (GAT)_, and _GraphSAGE_, each differing in how they aggregate neighbor information (summing, averaging, attention-weighted, etc.). But fundamentally, all these models perform neighbor-to-neighbor communication. _Zhou et al. (2021)_ provide a comprehensive review of GNN methods and note that GNNs are capable of capturing complex relationships in graph data through iterative information exchange among nodes​

[arxiv.org](https://arxiv.org/abs/1812.08434#:~:text=Graph%20neural%20networks%20,open%20problems%20for%20future%20research)

. GNNs have been used for tasks like node classification, link prediction, and even clustering​

[journalofbigdata.springeropen.com](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-023-00876-4#:~:text=to%20learn%20from%20graph%20data,classification%2C%20link%20prediction%2C%20and%20clustering)

, and they are increasingly being applied to optimization problems on graphs (including VRPs) to learn heuristics or improve solution quality.

In summary, Graph Neural Networks extend deep learning to graph-structured inputs by using a **message passing mechanism to aggregate information from neighboring nodes**, thereby producing enriched node (and graph) representations​

[arxiv.org](https://arxiv.org/abs/1812.08434#:~:text=Graph%20neural%20networks%20,open%20problems%20for%20future%20research)

. This allows ML models to effectively reason about relational data. As VRPs are naturally defined on graphs (road networks or complete distance graphs between customers), GNNs provide a relevant tool in the state-of-the-art approaches, enabling ML models to respect and exploit the problem’s structure. In later sections, we will see instances of GNNs aiding route construction or evaluation, underscoring the importance of this concept in modern ML-based VRP research.

### Concluding Remarks

The concepts introduced in this chapter – spanning ML paradigms (supervised, unsupervised, and reinforcement learning) and key techniques (from neural networks and SVMs to clustering, decision trees, and GNNs) – form the foundation for understanding the **machine learning-based approaches to the Vehicle Routing Problem** reviewed in this thesis. While we have kept the discussion at a high level, the intention is that terms like “autoregressive model,” “Q-learning,” or “graph neural network” are now clear enough to contextualize their use in the VRP literature. In the next chapters, we will see how researchers leverage these ML concepts and tools, often in creative ways, to tackle the challenges of VRPs. For instance, we will encounter supervised learning used to approximate VRP solution quality, unsupervised techniques to identify customer clusters, and deep reinforcement learning algorithms that construct routes. Equipped with the fundamental concepts from this chapter, a reader will be better prepared to follow and appreciate those state-of-the-art contributions.