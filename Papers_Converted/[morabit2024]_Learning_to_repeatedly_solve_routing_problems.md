## S P E C I A L I S S U E ARTICLE

## Learning to repeatedly solve routing problems

## Mouad Morabit 1,2,3 Guy Desaulniers 1,2 Andrea Lodi 3,4

1 Department of Mathematics and Industrial

Engineering, Polytechnique Montréal, Montréal, Quebec, Canada 2 GERAD, Montréal, Quebec, Canada 3 CERC, Polytechnique Montréal, Montréal, Quebec, Canada 4 Jacobs Technion-Cornell Institute, Cornell Tech, New York, New York, USA

## Correspondence

Andrea Lodi, CERC, Polytechnique Montréal, Montréal, QC, Canada.

Email: andrea.lodi@cornell.edu

In the last years, there has been a great interest in machine-learning-based heuristics for solving NP-hard combinatorial optimization problems. The developed methods have shown potential on many optimization problems. In this paper, we present a learned heuristic for the reoptimization of a problem after a minor change in its data. We focus on the case of the capacited vehicle routing problem with static clients (i.e., same client locations) and changed demands. Given the edges of an original solution, the goal is to predict and fix the ones that have a high chance of remaining in an optimal solution after a change of client demands. This partial prediction of the solution reduces the complexity of the problem and speeds up its resolution, while yielding a good quality solution. The proposed approach resulted in solutions with an optimality gap ranging from 0% to 1.7% on different benchmark instances within a reasonable computing time.

## KEYWORDS

heuristics, machine learning, reoptimization, routing

## 1 INTRODUCTION

In the recent years, the idea of integrating machine learning (ML) and combinatorial optimization (CO) has been greatly explored. CO problems are NP-hard and there are generally two approaches to solve them. The exact methods that are guaranteed to obtain an optimal solution but can be computationally very expensive for large instances, and the heuristic methods that trade off the optimality of the solution for a reasonable computing time. The idea of leveraging ML for the development of new heuristics has shown potential in many CO problems such as traveling salesman problem 1 (TSP), capacited vehicle routing problem (CVRP), and so forth. It is true that most of these learned heuristics do not outperform highly optimized and specialized CO algorithms, especially for problems that have been extensively studied in the literature. Nevertheless, the ideas behind them provide a certain flexibility for adjustments and applications to other problems for which no good heuristic exists, or they can be integrated in already existing algorithms to speed them up.

In this paper, we focus on CO applications in which a problem is repeatedly solved, for example, daily or hourly, or even within a shorter interval, by changing neither its structure (e.g., its constraints) nor even its size (i.e., its variables), but only the data that defines each instance solved in the specific time interval. This is the case of applications in which the infrastructure whose operations must be optimized does not change, for example a fleet of vehicles that deliver goods or the power plants producing energy, but the demand of goods or energy changes. And it is also the case of real-time changes to the data due to disruptions in the infrastructure, for example arcs disappearing from a network (i.e., their capacity going to 0) due to accidents.

Applications of this type might be difficult to solve in reasonable amount of time, or, more precisely, each instance in isolation might require a significant computational effort even if the solution method has been designed after intensively studying the characteristics of the CO problem. This is the theoretical consequence of NP-hardness, and, on the practical side, it is due to the fact that the solution methods are largely designed to be agnostic to the data.

1 The traveling salesman problem calls for finding the shortest unique tour visiting a set of points in a metric space.

![Image](image_000000_d5f9ea3ef9d1ae02331059f91a86918336b1e9a5c591ab8b79e11ab4c080895b.png)

The goal of this paper is to propose a learned heuristic allowing a fast reoptimization of a CO problem after a slight modification of its data. In other words, we put ourselves in a way more restrictive context with respect to the use of ML for CO problems: we do not want to leverage ML to devise a heuristic that produces good feasible solutions for all, say, TSP instances within the same distribution. We are settling for a lesser objective, that is, that of learning what can be (more or less with high probability) safely left unchanged in the solution of a reference instance of a CO problem when the data of the instance are perturbed. Indeed, the intuition is that given an instance and its solution S 1 , if the instance is reoptimized after a slight change in its data, the new solution S 2 will have a significant overlap with S 1 , while only some (minor) parts of the solutions will be different. Therefore, instead of reoptimizing from scratch, the goal is to predict the parts of the solution that have a high probability of remaining the same. The corresponding variables can be fixed, thus reducing the search space and accelerating the resolution of the problem, almost independently of the solution method applied.

To provide a concrete example, let us consider the CVRP, which is the problem where we will apply the method proposed in this paper. The goal of the CVRP is to construct vehicle routes in order to serve geographically-dispersed clients while minimizing the travel costs and respecting vehicle capacity. Let us take the example of a delivery company that solves CVRPs on a daily basis. For a given day, the company observes that the clients are the same as in the instance solved the previous day (i.e., same client locations) and that only some clients have a different demand. After the optimization of the problem, a similarity between the solutions is noticed. Given the optimal solution (or a heuristic one) already in hand and the new demands, the objective would be to predict and fix the sequences of edges that have a high probability to remain the same. In case of a graph-structured problem like the CVRP, it is also possible to reduce the size of the network by aggregating the nodes/edges of the fixed sequences, therefore accelerating the resolution of the problem and reducing its complexity. Note that the predictions obtained by the learned model are not necessarily 100% accurate, misclassifications may occur and thus affect the quality of the solution. The goal is to find a good compromise between the quality of the solution (i.e., optimality gap) and the computing time.

Weacknowledge that the concept of 'slight' modifications is not mathematically defined: one could argue that, if the number of points is fixed, any TSP instance is a slight modification of another because constraints and variables remain the same, while only the objective function changes due to the change in the location of the points. Our approach targets a more restricted context in which a reference instance-as well as its solution-exists, and the learning method is used to predict the effect that data modifications to that specific instance have on that specific solution. In other words, our approach cannot be applied if the similarity between S 1 and S 2 discussed above does not exist. So, again in the TSP context, one should assume that only a few points change location and these changes are, in turn, small-a very restrictive assumption. Nevertheless, one can think of several CO contexts where naturally (slight) modifications to the problem data lead to similar solutions after reoptimization, especially for problems that are solved repeatedly and for which a data set is already available (e.g., the unit commitment problem in which the power plants producing energy are always the same but the demand changes daily or hourly). In this paper, we will only consider as a proof of concept the case of the CVRP with changing demands and fixed customers. We believe that the method offers some flexibility and has the potential to be applied to other problems as well as being integrated into existing algorithms.

The remainder of the paper is organized as follows. In Section 2, we present some recent work on using ML for solving CO problems or accelerating their solution process. Section 3 is devoted to presenting the CVRP, with a focus on the methods we consider to solve it. In Section 4, we cover all the details of the method we propose. Section 5 reports our computational results. Finally, conclusions are drawn in Section 6.

## 2 RELATED WORK

In the literature, several heuristics incorporating ML models for solving NP-hard CO problems were explored (see, the recent surveys [5, 8, 18]). The proposed learning methods mostly fall into one of two categories. In the first category, supervised learning methods (examples of the imitation learning paradigm) are algorithms that learn from data and try to mimic an expert. The data is given to the learner as a pair of features and expected outputs (or labels) and the learner tries to find patterns in the data while optimizing a performance measure. Generally, the aim of this approach is to replace known expensive computations by fast approximations (e.g., for our case, the expensive computation corresponds to reoptimizing the problem from scratch). In the second category, we find the reinforcement learning (RL) algorithms that apply a 'learning-by-experience' paradigm. Instead of giving the learner the data on which to learn, RL algorithms explore the decision space by interacting with its environment in order to achieve a certain goal. In response to a decision (i.e., an action), the learner receives a real-valued reward. The goal is to find the best decisions to make at each state while maximizing the expected rewards. This learning approach has the advantage of not requiring any data and has generally shown a better generalization, that is, the ability of continuing to be effective when the problems change for example in size or even better in data distribution.

Several studies have tried to tackle the TSP, the most classical CO problem where the goal is to visit a set of nodes exactly once with a single vehicle while minimizing the travel distance. For the supervised approaches, we mention the work of [15], where a learned heuristic method is presented. The method is based on a graph convolutional network model that takes the entire graph as an input and outputs an adjacency matrix with associated edge probabilities, which are then used to build a valid tour using a beam search algorithm. The authors report good results on fixed size instances. However, a very poor generalization is noticed when testing the models on instances of different size. Instead, the methods based on RL have shown more potential. The idea of using RL to solve CO problems was explored in [4] with an application to the TSP and the knapsack problem. Further studies were then conducted, such as [17], where the authors present an encoder-decoder model based on the attention mechanism [27]. In a similar line, [24] present a framework focusing on solving the CVRP, which can be considered as a generalization of the TSP for the case with multiple vehicles. The results reported by the authors show a better performance when compared to the OR-Tools solver and other heuristic algorithms. Other researchers contributed to the methods for solving the broader class of graph-structured CO problems. [11] initiated the idea of learning on graphs, and the works of [20] and [23] enhanced further the scalability to larger graphs. These methods have been applied to various NP-hard graph problems such as the minimum vertex cover, maximum clique, influence maximization problem, and so forth.

Unlike the previous works that seek to build an end-to-end solution to the different problems, other methods focus on using ML to guide and accelerate the solution process. Since we pay special attention to mixed-integer programs (MIP), it is worth mentioning the various projects involving ML in the context of branch and bound [2, 13, 16, 29], many of which seek to learn a branching policy imitating the strong branching strategy. Another potential use of ML is embedding a learned model in MIP solvers in order to decide if a decomposition of the problem is beneficial or not [19], or if it is favorable to linearize the quadratic part of a mixed-integer quadratic program [7].

Other learning-based methods have proven to be very effective on problems that are solved repeatedly, especially when the input data changes only slightly, which is the key idea of our project. These methods can exploit existing data from previous solutions in order to speed up the resolution of similar unseen instances. [28] exploit the idea and apply it on the security-constrained unit commitment, a problem occurring in power systems and electricity markets. The authors report high speedups on computing time, up to 10 times faster than solving the problem from scratch, and without a noticeable loss in solution quality. Along the same lines, [21] consider an application to the facility location problem. They seek to estimate the proportion of a solution that has a high probability of remaining unchanged after a perturbation. An additional constraint is added to the original formulation according to the predictions obtained by a regression model. Both [28] and [21] leverage ML for speeding up the resolution of repeatedly solved problems and do not seek to build an end-to-end solution, which is similar to our case. As opposed to [28], our method is related to that of [21] in the sense that both assume a reference solution to which changes (perturbations) are applied. However, the main difference lies on the fact that our approach revolves around fixing parts of the reference solution (i.e., edges) instead of estimating a bound on the number of changes without specifying which variables to set. Other than that, the CVRP remains a quite different problem and the approaches developed in this paper have the potential to be extended to other CVRP variants and also to other routing problems.

## 3 THE CAPACITED VEHICLE ROUTING PROBLEM

The CVRP is a CO problem that has been studied for many years, resulting in several exact and heuristic methods to solve it. It is classified as an NP-hard problem and remains a difficult problem to solve to optimality even with just a few hundred clients. Given a fleet of vehicles assigned to a depot, the problem consists in determining a set of possible routes (i.e., one route per vehicle used) to deliver goods to a set of dispersed clients while minimizing the travel costs. A route starts from the depot and visits a sequence of clients before returning back, and is considered feasible if the total amount of goods delivered does not exceed the vehicle capacity Q .

For solving this problem, we consider the algorithms based on column generation (CG; [12]), which is an exact iterative method for solving large linear programs. To ensure the obtention of integer solutions, CG is often embedded in a branch-and-bound framework where the linear relaxation of the problem is solved at each node using CG. In this case the method is referred to as branch-and-price (B&amp;P) and is considered the state-of-the-art exact method for solving the CVRP. But due to the complexity of the problem, solving large instances to optimality can be computationally expensive. Note that fixing sequences of edges may seem to be more suitable for an edge-flow formulation solved by a branch-and-cut solver. However, it is also very suitable for a B&amp;P method because the CVRP network structure is exploited in the CG pricing problem as discussed below.

Finally, note that our approach does not necessarily require learning from optimal solutions. Since we want to apply it on relatively large instances (i.e., with 100 clients or more), solving the instances to optimality in order to collect data for the training phase can be highly time consuming. Therefore, we chose a heuristic to solve the instances in the data collection phase,

but during the evaluation phase, we exploit an exact B&amp;P algorithm to solve new instances. In the next sections, we present the problem formulation along with both exact and heuristic algorithms used.

## 3.1 CVRP formulation

In this section, we formulate the CVRP as a set partitioning problem. Let C be the set of clients to be serviced, Ω the set of all feasible routes and cr the cost of a route r ∈ Ω . We define a r i as a binary parameter equal to 1 if client i ∈ C is serviced by route r ∈ Ω and 0 otherwise. Let /u1D703 r be a binary decision variable equal to 1 if route r is part of the solution and 0 otherwise. The problem can therefore be formulated as follows:

$$( P ) & \min _ { \stackrel { \theta } { \theta } } \sum _ { r \in \Omega } c _ { r } \theta _ { r } & ( 1 ) & \frac { \stackrel { \bar { \Theta } } { \Theta } } { \Theta } \\ & \frac { \stackrel { \bar { \Theta } } { \Theta } } { \Theta }$$

$$\mathfrak { s. t. } \sum _ { r \in \Omega } a _ { i } ^ { r } \theta _ { r } & = 1, \quad \forall i \in C, & ( 2 ) & = \frac { \mathfrak { s. } } { \mathfrak { s. } } \\ \frac { \mathfrak { s. } } { \mathfrak { s. } }$$

$$\theta _ { r } \in \{ 0, 1 \}, \ \forall r \in \Omega, & & ( 3 ) & & \frac { \tilde { \Xi } } { \tilde { \Xi } }$$

where the objective (1) minimizes the total cost of the routes. Constraints (2) ensure that each client is visited exactly once and constraints (3) are the binary requirements on the decision variables /u1D703 r .

One can notice that for large instances, the size of the route set | Ω | becomes prohibitively large and it would not be possible to enumerate all the variables of the problem. This is why a CG-based algorithm is used. The goal is to start with a subset of variables and generate potentially improving columns when necessary.

## 3.2 Exact branch-and-price algorithm

B&amp;Palgorithms [3] are considered state-of-the-art exact algorithms for solving a variety of optimization problems (e.g., routing, scheduling, … ). B&amp;P is based on the branch-and-bound method in which the linear relaxation at each node is solved using CG. The CG process is iterative and consists in alternating between the resolution of a restricted version of the original linear relaxation, called a restricted master problem (RMP), and a pricing problem (PP). The goal of the PP is to find new improving columns of negative reduced cost (for a minimization problem) that can be added to the RMP. The CG process stops when no such columns are found. A branching then occurs and the branch-and-bound tree exploration continues. Optionally, cutting planes can also be added to strengthen the relaxation at each node, resulting on what is called a branch-cut-and-price method.

## 3.2.1 The restricted master problem

The RMP corresponds to the linear relaxation of the formulation (1)-(3) but limited to only a subset  ⊂ Ω of the variables. It is solved at each CG iteration, yielding a pair of primal and dual solutions ( /u1D703 , /u1D70B ) . The dual values ( /u1D70B i ) i ∈ C associated with the constraints (2) are then used to find routes r ∈ Ω /uni29F5  of negative reduced cost by solving the PP. If none exist, the CG process stops and the solution to the current RMP is thus optimal for the whole linear relaxation. Otherwise, the routes are added to the RMP (i.e., to the subset  ) which is then reoptimized.

## 3.2.2 The pricing problem

The PP can be defined as min r ∈Ω { cr -∑ i ∈ C a r i /u1D70B i } . For many applications, especially routing and scheduling problems, this problem can be modeled as an elementary shortest path problem with resource constraints (ESPPRC; [14]) where the goal is to find a least-cost path between the source and destination nodes while visiting the nodes at most once (i.e., elementarity requirements) and respecting the resource constraints. For the CVRP, the only resource is the load, and a path is considered feasible if the load does not exceed the vehicle capacity. This problem can be defined over a graph G = ( V A , ) , where V is the set of nodes representing the clients, in addition to the depot nodes s and t , that is, the source and destination nodes, respectively. The set A represents the arcs, where each arc has an associated cost cij , ( i , j ) ∈ A . Hence, the cost of a path in G is given by the sum of the costs cij of its arcs.

In order to take into account the dual values obtained by the RMP, at each iteration and for each arc ( i , j ) ∈ A , a modified cost cij = cij -/u1D70B i is used instead, where /u1D70B i are the duals associated with constraints (2) and /u1D70B s = 0. This guarantees that the cost of a feasible route in the network is equal to its reduced cost:

$$\bar { c } _ { r } = c _ { r } - \sum _ { i \in C } a _ { i } ^ { r } \pi _ { i } = \sum _ { ( i, j ) \in A } c _ { i j } b _ { i j } ^ { r } - \sum _ { i \in C } a _ { i } ^ { r } \pi _ { i } = \sum _ { ( i, j ) \in A } ( c _ { i j } - \pi _ { i } ) b _ { i j } ^ { r } = \sum _ { ( i, j ) \in V } \bar { c } _ { i j } b _ { i j } ^ { r } & & ( 4 ) & & \frac { \frac { \pi } { 2 } } { \frac { \pi } { 1 } }$$

where b r ij is equal to 1 if arc ( i , j ) ∈ A is traversed in route r ∈ Ω , 0 otherwise.

Because the ESPPRC is an NP-hard problem, it is often replaced by one of its relaxations as discussed in [10] that can be efficiently solved using dynamic programming, namely, a labeling algorithm. In this algorithm, a label represents a partial path in G (not to confuse with the notion of label in ML). The algorithm starts with an initial label representing the trivial path containing only the source node s , which gets extended forward along the outgoing arcs until reaching the destination node t . A new label is created at each extension if it yields a feasible path. At the end of the algorithm, the labels at the destination node t representing negative reduced cost routes are used to build the new columns that are added to the RMP. Generally, several routes are added at once, which is known to speed up the solution process and to reduce the number of CG iterations.

## 3.2.3 Branch-and-price implementation

In this work, we consider using VRPSolver [25], which is a generic implementation of an exact branch-cut-and-price method for VRP problems. The advantage of using VRPSolver lies in the fact that it combines several algorithms and acceleration techniques introduced by several authors, for example, ng-routes, path enumeration, bi-directional labeling, stabilization, and so forth. Implementing these techniques from scratch would be otherwise very time-consuming. The authors report excellent performance on several benchmark instances of various CVRP variants (e.g., with time windows, heterogeneous fleet, multiple depots, pickups and deliveries, etc.). For more information about the implementation and the algorithms included in VRPSolver, the reader is referred to [25].

## 3.3 Heuristic algorithm for data collection

To train a ML model, we need a large quantity of data, including a large number of solutions from various CVRP instances in our case. However, as previously mentioned, the CVRP is difficult to solve to optimality and using an exact B&amp;P algorithm for solving a large number of instances can be too time consuming. Therefore, rather than using an exact algorithm to produce data for training, we have decided to apply a recent heuristic called FILO [1], for Fast Iterated Localized Search Optimization , that has proved to be highly competitive on several benchmark instances. For the X instances introduced by [26], it produced solutions with optimality gaps of less than 0 1 . % on average, making it a very good candidate for our experiments.

The FILO heuristic is based on the iterated local search paradigm and is specifically designed to solve large-scale CVRP instances. The algorithm starts by constructing an initial feasible solution using an adaptation of the savings algorithm [9]. It is followed by an optional step that aims at reducing the number of vehicles used in the initial solution, if the latter is larger than a computed estimate (using a bin-packing greedy algorithm). The algorithm then proceeds to the core optimization step, which mainly executes a sequence of ruin-and-recreate steps. The ruin step removes a certain number of vertices by means of a random walk of a given length. Then, the recreate step reinserts the removed vertices while trying to improve upon the best solution found. The algorithm stops after a determined number of iterations.

## 4 METHODOLOGY

The goal of this project is to accelerate the reoptimization of repeatedly solved CO problems for which there is only a slight change in the problem data. Let  o and /uniE23F o be a problem instance and a computed good-quality solution, respectively. Furthermore, let  m be a modified instance obtained by applying minor changes to  o and for which no solution is known. Instead of optimizing  m from scratch, the objective is to identify the parts of /uniE23F o that have a high probability of also being part of a solution to  m denoted by /uniE23F m .

In this paper, we focus on the CVRP where the locations of the clients are the same but the demands are slightly different. Because we assume that the fleet of vehicles is unlimited and the travel costs are symmetric, we consider for the rest of this paper an undirected graph and denote by E ( /uniE23F o ) the set of edges used in the known solution /uniE23F o to the original instance  o . Since the solution consists of vehicle routes, if we consider E ( /uniE23F o ) as the set of edges used in a solution /uniE23F o already in hand, the method aims at predicting the edges e ∈ E ( /uniE23F o ) that have a high chance to also be part of /uniE23F m . By fixing these edges, a significant reduction of the problem complexity can be achieved, thus greatly reducing the computing time. Let us take the example illustrated in Figure 1, where we can observe the following: Figure 1A represents a solution /uniE23F o to an original CVRP instance involving 110 nodes. The central node corresponds to the depot and the other nodes represent the clients. Figure 1B shows a solution /uniE23F m to an instance  m obtained by randomly changing the demands of 20 % of the clients of  o (the clients with a changed demand are marked with a star node). By comparing the two figures, one can notice a significant similarity between the two solutions. Assuming we do not have yet the solution /uniE23F m , if we succeed in predicting and fixing the edges e ∈ E ( /uniE23F o ) that will be part of /uniE23F m , a partial solution can be provided to the solver before starting the optimization. This is shown in Figure 1C, where the overlapping parts of the two solutions are highlighted, that is, the edges e ∈ E ( /uniE23F o ) ∩ E ( /uniE23F m ) . Furthermore, it is also possible

(C)

![Image](image_000001_c4b10375f226f61f69db6104b15b35e98f3f22b4c004328590001b9c425cce45.png)

(D)

FIGURE 1 Overview of the different steps of the method.

to reduce the size of the network by aggregating the sequences comprised of two or more edges into a single one as shown in Figure 1D. As a result, the number of nodes is reduced from 110 to only 54.

Since the predictions obtained by the ML model are not always accurate and errors may occur, fixing the wrong edges can affect the quality of the solution obtained. In fact, fixing many edges implies shorter computing times but more chances to fix the wrong ones. On the other hand, if the number of fixed edges is small, there is a greater chance of obtaining a better quality solution though with a higher computing time. By controlling the number of fixed edges (e.g., by tuning the hyperparameters of the model), it is possible to find the right compromise between the quality of the solution and the computing time.

## 4.1 Data collection

Wechose to address this learning problem using a supervised learning approach. More precisely, a binary classification model is employed. The first step in the process is to collect enough data for the training. Given a tuple of original and modified instances and their solutions (  o , /uniE23F o ,  m , /uniE23F m ) , a labeled dataset  = {{ x e , ye } ∀ | e ∈ E ( /uniE23F o )} is built where each entry represents an edge in the original solution, the vector x e ∈ R n corresponds to the edge features (i.e., input), where n is the number of features and ye = { 0 1 , } is the desired output (i.e., label). One can notice that we are only interested in the edges of /uniE23F o and not all the edges of the graph.

The labels. Since we tackle this problem in a supervised manner, we need both solutions /uniE23F o and /uniE23F m to build the dataset, that is, we need to give both the input and the desired output to the learner. The labels are assigned by simply checking the overlapping edges between the solutions /uniE23F o and /uniE23F m as follows:

$$y _ { e } = \begin{cases} 1 & \text{ if } e \in E ( S _ { o } ) \cap E ( S _ { m } ) \\ 0 & \text{ otherwise} \end{cases}, \quad e \in E ( S _ { o } ). & \text{ (5)} & \text{ (3)} & \text{ (2)} & \text{ (3)} & \text{ (2)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \text{ (3)} & \end{cases}$$

The features. The features x e represent the characteristics of each edge e = ⟨ i , j ⟩ in the original solution. The extracted features are the following:

- · The ( x y , ) coordinates of both nodes i and j ;
- · The cost of edge ce ;
- · The old and new demands of nodes i and j (the demand is set to 0 for the depot nodes s and t );
- · The distance between the depot and nodes i and j ;
- · A boolean value indicating whether the edge is a depot edge, that is, equal to 1 if either i or j is a depot node;
- · A boolean value indicating whether the client i or j , or both, have a changed demand;
- · The rank of i with respect to j (and vice versa) according to the neighbor distances, for example, if j is the nearest neighbor to i , then its rank is 1, if j is the second closest neighbor to i then its rank is 2, and so on.

Note that since we are working on a symmetric version of the CVRP and that the order of the entries in the vector x e is important, we always assume that i &lt; j .

## 4.2 MLprediction

Once the dataset is in hand, we apply common ML practices such as data preprocessing (i.e., normalization, encoding) and data splitting (i.e., dividing the data into a training, a validation and a test set), and so forth. As mentioned before, the prediction task corresponds to a binary classification problem and different ML models can be considered, for example, Random Forest, SVM, Neural network. In this section, we assume that we already have a trained predictive model that takes as input the edge features x e and outputs the predictions ̂ ye and we wish to know what to do with those predictions afterwards (the comparison of which ML model is best performing is discussed in the next section).

## 4.2.1 ML-based edge fixing

Let G = ( V E , ) be an undirected graph, where V is the set of nodes including the depot nodes and E the set of edges. Let ce , e ∈ E be the cost of edge e = ⟨ i , j ⟩ . The edge flow can be written in terms of the master problem (1)-(3) variables as

$$x _ { e } & = \sum _ { r \in \Omega } b _ { e } ^ { r } \theta _ { r }, & ( 6 ) & = \sum _ { \substack { \frac { \sqrt { 2 } } { 2 } \\ \frac { \sqrt { 2 } } { 2 } } }$$

where b r e ∈ { 0 1 , } indicates whether edge e ∈ E is part of a route r ∈ Ω . Therefore, an edge can be fixed by adding a new constraint to the master problem. For each edge in the original solution /uniE23F o , the flow is set to 1 depending on the predictions obtained by the ML model, namely

$$x _ { e } & = 1, \quad \forall e \in E ( S _ { o } ) \ \colon \, \hat { y } _ { e } = 1. & ( 7 ) & \quad \frac { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \overline { \over } } } } } } } } } } } } } } } }$$

## 4.2.2 Infeasibility case

Sometimes fixing edges can lead to an infeasible restricted problem. This may only occur in the case when the total demand of a sequence of clients fixed by the ML model exceeds the vehicle capacity Q . For each edge, in addition to the output ̂ ye returned by the model, it is possible to obtain its probability estimate ̂ p e of being fixed. Generally, the model assigns the class 1 if the probability estimate ̂ p e is greater than 0.5 and 0 otherwise. For a sequence of clients whose sum of the demands exceed Q , a possible solution would be to identify and unfix the edges with lowest probabilities, until obtaining one or several feasible subsequences.

Let Gy ̂ = ( Vy ̂ , Ey ̂ ) be the graph obtained by keeping only the fixed edges and their corresponding nodes, that is, Ey ̂ = { e ∈ E | ̂ ye = 1 } and Vy ̂ = { i ∈ V | ∃ ⟨ k l , ⟩ ∈ Ey ̂ ∶ i = k ∨ i = } l . Depending on the predictions obtained by the model, this graph may contain sequences of multiple nodes as shown in Figure 1C. A sequence corresponds to a connected component in Gy ̂ and can be defined as p = ( v p 1 , v p 2 , … , v p | p | ) , vi ∈ Vy ̂ , where | p | is its length. Note that since the graph is undirected, the sequence can start at either of its ends. Let E p ( ) = {{ i , i + 1 } | i ∈ { 1 2 , , … , | p | -1 }} be the set of edges of sequence p and S Gy ( ̂ ) the

set of all sequences in Gy ̂ . We denote by /u1D451 i the demand of node i ( /u1D451 s = /u1D451 t = 0 . The steps followed to identify and to deal with ) the infeasible sequences are described in Algorithm 1.

## Algorithm 1. Infeasibility check.

```
                                                                                                                                                                                                        |
                                                                                                                                                                                                       
                                                                                                                                                                                                       |                                                                                                                                                                                                        \
                                                                                                                                                                                                      
	                                                                                                                                                                                                        <?                                                                                                                                                                                                        </?                                                                                                                                                                                                       <!                                                                                                                                                                                                        >!                                                                                                                                                                                                       <:                                                                                                                                                                                                       	                                                                                                                                                                                                       <                                                                                                                                                                                                        :                                                                                                                                                                                                       <?                                                                                                                                                                                                      

                                                                                                                                                                                                       <?:                                                                                                                                                                                                       ?                                                                                                                                                                                                        +
```

The algorithm takes as input the graph Gy ̂ and starts by initializing the variable infeasibilityDetected to False (Step 3). Then, it proceeds by looping over the sequences of the set S Gy ( ̂ ) in Step 4. For each sequence p , if the total demand of the clients exceeds the capacity Q of a vehicle (Step 5), the edge with the lowest probability estimate is identified (Step 6), then unfixed and removed from the set of edges Ey ̂ (Steps 7 and 8). This procedure is repeated until there are no more infeasible sequences (Steps 2-12). Thus, if new sequences are created as a result of unfixing edges, their feasibility is checked in subsequent iterations.

## 4.2.3 Network reduction

Once there are no more infeasible sequences, it is possible to make an improvement that can further accelerate the optimization, which consists in reducing the number of nodes and edges in the network. The simple (and well-known) idea is to shrink the sequences of three nodes (i.e., two edges) or more into a single edge, while making sure that the cost and the demands are updated accordingly.

For a sequence p = ( v p 1 , … , v p | p | ) ∈ S Gy ( ̂ ) with | p | ≥ 3, the goal is to remove all intermediate nodes v p 2 , … , v p | p | -1 and link directly the two ends of the sequence by adding the edge enew = ⟨ v p 1 , v p | p | ⟩ . Its cost is updated to the cost of the whole sequence, that is, ce new = ∑ e ∈ ( ) E p ce , and the demand of the removed nodes ∑| | p -1 i = 2 /u1D451 v p i is added to the demand of either v p 1 or v p | p | . Lastly, the edge is fixed by setting the corresponding flow variable xe new = 1. By doing so, traversing the edge enew becomes equivalent to traversing the sequence p . The reduction of the network size depends on the number of sequences and their lengths, in some cases a significant reduction can be obtained, as illustrated in Figure 1D.

## 4.2.4 Method summary

By putting all the pieces together, Algorithm 2 summarizes the different steps of our method. Given the initial data (  o ,  m , /uniE23F o ) , the algorithm starts by extracting the edge features yielding the features matrix X (Step 1), then obtaining the predictions ̂ y of the ML model and the probability estimates ̂ p in Steps 2 and 3. The graph Gy ̂ representing the edges to fix is built (Steps 4 and 5) and the procedure resolve\_infeasibility described in Algorithm 1 is called to check for any infeasible sequences. The algorithm proceeds by aggregating the sequences in S Gy ( ̂ ) by connecting each sequence ends (Step 9), updating the cost and the demand (we added the demands of the intermediate nodes to one of the two ends) in Steps 10 and 11, then removing the intermediate nodes and their adjacent edges (Steps 12 and 13). Any removed node from Gy ̂ must be removed from the original graph also. This is achieved by updating G to the induced subgraph G Vy [ ̂ ] . At this point, the graph G corresponds to the original complete graph minus the removed nodes from the network reduction step. Finally, the master problem is initialized and solved after adding the flow constraints for each fixed edge (Steps 17-21).

## 5 COMPUTATIONAL EXPERIMENTS

This section start by describing the CVRP instances we used and the data generation process. Next, we present the details about the ML phase. Finally, the results of our heuristic method on different benchmark instances are reported. All the experiments were conducted on a Linux machine with an Xeon(R) Gold 6142 CPU @ 2.60GHz and 512GB of RAM.

## 5.1 CVRP instances

The instances used are based on the X benchmark dataset introduced by [26]. Each instance has different characteristics, among others, the depot position, the client positioning (e.g., clustered or randomly dispersed), the vehicle capacity and the demand distribution. Since we are interested in modifying the demands of the clients, by taking a closer look at the instances, we find different intervals where all demands are drawn uniformally from each distribution. The demands can range from: (a) [1-10], (b) [5-10], (c) [1-100], (d) [50-100], (e) [1-50] for clients located in an even quadrant and [50, 100] for the others. There are also instances with unit demands but we exclude them since they are less relevant in our case. Table 1A describes the instances we picked for our experiments. The instance names are written in the form 'X-n[ nno /u1D451 es ]-k[ nvehicles ]', where nno /u1D451 es is the number of nodes including the depot and nvehicles is an estimate of the number of vehicles required to service all clients.

## 5.2 Data generation

For each instance described in Table 1A, we generate a set of modified instances by randomly changing the demands of Nc % of the clients. The new demand of each of these clients is chosen randomly in the interval [ /u1D451 i - Δ /u1D451 , /u1D451 i + Δ ] /u1D451 , where /u1D451 i is the original demand of the client i and Δ /u1D451 a parameter controlling the interval width. In order to analyze the impact of the demand changes on the performance of the heuristic and the predictions, we used different values of Nc ∈ { 10 20 30 . As for , , } Δ /u1D451 , since each instance has a different demand distribution, we created three classes of intervals: Small (S), Medium (M) and Large (L), controlled by the value of Δ /u1D451 as shown in Table 1B.

TABLE 1 Instance characteristics and demand changes.

| (A) CVRP instances from the X benchmark instances.                    | (A) CVRP instances from the X benchmark instances.                    | (A) CVRP instances from the X benchmark instances.                    | (A) CVRP instances from the X benchmark instances.                    | (A) CVRP instances from the X benchmark instances.                    |
|-----------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|-----------------------------------------------------------------------|
| Instance name                                                         | Demand distribution                                                   | Demand distribution                                                   | Avg. route size                                                       | Avg. route size                                                       |
| X-n101-k25                                                            | [ 1 - 100 ]                                                           | [ 1 - 100 ]                                                           | 4.0                                                                   | 4.0                                                                   |
| X-n106-k14                                                            | [ 50 - 100 ]                                                          | [ 50 - 100 ]                                                          | 7.5                                                                   | 7.5                                                                   |
| X-n110-k13                                                            | [ 5 - 10 ]                                                            | [ 5 - 10 ]                                                            | 8.4                                                                   | 8.4                                                                   |
| X-n125-k30                                                            | Qua /u1D451 rant                                                      | Qua /u1D451 rant                                                      | 4.1                                                                   | 4.1                                                                   |
| X-n129-k18                                                            | [ 1 - 10 ]                                                            | [ 1 - 10 ]                                                            | 7.1                                                                   | 7.1                                                                   |
| X-n134-k13                                                            | Qua /u1D451 rant                                                      | Qua /u1D451 rant                                                      | 10.2                                                                  | 10.2                                                                  |
| X-n139-k10                                                            | [ 5 - 10 ]                                                            | [ 5 - 10 ]                                                            | 13.8                                                                  | 13.8                                                                  |
| X-n143-k07                                                            | [ 1 - 100 ]                                                           | [ 1 - 100 ]                                                           | 20.3                                                                  | 20.3                                                                  |
| (B) /u1D6AB /u1D485 values used depending on the demand distribution. | (B) /u1D6AB /u1D485 values used depending on the demand distribution. | (B) /u1D6AB /u1D485 values used depending on the demand distribution. | (B) /u1D6AB /u1D485 values used depending on the demand distribution. | (B) /u1D6AB /u1D485 values used depending on the demand distribution. |
|                                                                       | /u1D6AB /u1D485 (interval size)                                       | /u1D6AB /u1D485 (interval size)                                       | /u1D6AB /u1D485 (interval size)                                       | /u1D6AB /u1D485 (interval size)                                       |
| Demand distribution                                                   | S                                                                     | M                                                                     | M                                                                     | L                                                                     |
| [ 1 - 100 ]                                                           | 5                                                                     | 10                                                                    | 10                                                                    | 15                                                                    |
| [ 50 - 100 ]                                                          | 5                                                                     | 10                                                                    | 10                                                                    | 15                                                                    |
| [ 5 - 10 ]                                                            | 1                                                                     | 2                                                                     | 2                                                                     | 3                                                                     |
| Qua /u1D451 rant                                                      | 5                                                                     | 10                                                                    | 10                                                                    | 15                                                                    |
| [ 1 - 10 ]                                                            | 2                                                                     | 3                                                                     | 3                                                                     | 4                                                                     |

By combining the three different values of Nc and the three interval sizes, this results in nine different scenarios Φ = {10S, 10M, 10L, 20S, 20M, 20L, 30S, 30M, 30L}. We then proceeded as follows. For each instance in Table 1A and each scenario /u1D719 ∈ Φ , 100 modified instances are generated, where 95 of them are used for the ML phase (i.e., training, parameters tuning, etc.) and the remaining 5 for the optimization phase (i.e., when the ML model is incorporated in the CG algorithm). We therefore consider the eight instances of Table 1A as the original instances (i.e.,  1 o , … ,  8 o ) and, for each scenario /u1D719 , we generate 100 modified versions (i.e., (  i m ) 1 /u1D719 , … ( ,  i m ) 100 /u1D719 , i ∈ { 1 2 , , … , 8 } , /u1D719 ∈ Φ ). One ML-model is trained for each instance and for each scenario. The idea of training a model for each instance comes from the assumption that we have a specific instance that we solve repeatedly. Therefore we want a specific model for that particular instance. This makes a total of 8 instances × 9 scenarios × 100 = 7200 modified instances generated (and 8 × 9 = 72 ML models). As mentioned before (see Section 3.3), given the large number of instances, we decided to use the FILO heuristic of [1] during the data collection phase, whereas the exact B&amp;P algorithm is exploited during the evaluation phase. For each of the 7200 instances, we run the heuristic using 10 different random seed values for 1 000 000 iterations and the solution with the smallest cost is retained.

Once the solutions of the original and modified instances are obtained, for each instance i ∈ { 1 , … , 8 } and scenario /u1D719 ∈ Φ , the solutions are grouped in a set of tuples, that is, {(  i o , /uniE23F i o , (  i m ) 1 /u1D719 , ( /uniE23F i m ) 1 /u1D719 ) , … ( ,  i o , /uniE23F i o , (  i m ) 100 /u1D719 , ( /uniE23F i m ) 100 /u1D719 )} that are used to extract the edge features and labels as detailed in Section 4.1.

## 5.3 Machine learning phase

Before starting the training, common practices in ML are followed, starting by a pre-processing phase that consists of scaling and normalizing the data. The dataset (i.e., the data from the 95 instances) is then split into a training set, a validation set and a test set. The goal of the validation set is to tune the different hyperparameters, whereas the purpose of the test set is to compare different classification algorithms, such as logistic regression, K-nearest neighbors, random forest, artificial neural network (ANN), and so forth. According to the results obtained on the test set, the ANN model is the overall most robust model with accuracies ranging from 70 % to 88 % depending on the instance and the scenario /u1D719 ∈ Φ (more detailed results about the models performance are reported in the next section). The hyperparameter values used during the training of the ANN models are described in Table 2.

## 5.4 Optimization phase

In this section, we present the results obtained by our edge-fixing heuristic. Since a ML model is trained for each original instance and scenario, we report the ML model performance in this section as well. Tables 3-5 summarize the results obtained on the test instances. Each row corresponds to the average values obtained on the five test instances (for each original instance), whereas the details of each individual instance can be found in Appendix A. There is one table for each Nc value (i.e., 10, 20, and 30 for Tables 3,4, and 5, respectively). The first column corresponds to the interval used when changing the demands (i.e., S, M, and L, respectively), followed by the name of the original instance  o . Next, the average cost of the best solutions /uniE23F m

TABLE 2 Hyperparameters values of the ANN models.

| Hyperparameter      | Value            |
|---------------------|------------------|
| Learning rate       | 10 - 3           |
| Number of epochs    | 1000             |
| Epoch size          | 64               |
| Batch size          | 32               |
| NN Architecture     | 32 × 32 × 32 × 1 |
| Activation function | ReLU             |
| Output function     | Sigmoid          |
| Optimizer           | Adam             |
| Class weights       | Balanced         |

TABLE 3 Average results for scenarios with Nc = 10.

|          |            |          |                   | metrics   | metrics   | metrics   | MLmodel Edge-fixing   | MLmodel Edge-fixing   | MLmodel Edge-fixing   | MLmodel Edge-fixing   | Exact B&P   | Exact B&P   | Exact B&P   | DACT   | DACT   |
|----------|------------|----------|-------------------|-----------|-----------|-----------|-----------------------|-----------------------|-----------------------|-----------------------|-------------|-------------|-------------|--------|--------|
| Interval |  o        |  m cost | sim (  o ,  m ) | TNR       | TPR       | Accuracy  | %Reduc.               | Cost                  | Time (s)              | Gap                   | Cost        | Time (s)    | Ratio       | Cost   | Gap    |
| Small    | X-n101-k25 | 27 637   | 84%               | 71%       | 70%       | 70%       | 63% (29)              | 27 718                | 13                    | 0.29%                 | 27 635      | 211         | 22.4        | 28 184 | 1.98%  |
|          | X-n106-k14 | 26 376   | 85%               | 89%       | 60%       | 75%       | 53% (44)              | 26 436                | 25                    | 0.23%                 | 26 376      | 962         | 61.3        | 26 897 | 1.98%  |
|          | X-n110-k13 | 14 987   | 93%               | 100%      | 66%       | 83%       | 61% (46)              | 14 987                | 8                     | 0.00%                 | 14 987      | 265         | 36.7        | 15 159 | 1.15%  |
|          | X-n125-k30 | 55 613   | 63%               | 76%       | 73%       | 75%       | 55% (27)              | 55 683                | 235                   | 0.13%                 | -           | -           | -           | 58 581 | 5.34%  |
|          | X-n129-k18 | 28 765   | 62%               | 78%       | 72%       | 75%       | 53% (51)              | 28 982                | 78                    | 0.75%                 | 28 765      | 3479        | 53.4        | 29 697 | 3.24%  |
|          | X-n134-k13 | 10 888   | 80%               | 89%       | 54%       | 72%       | 45% (44)              | 10 917                | 77                    | 0.27%                 | -           | -           | -           | 11 284 | 3.64%  |
|          | X-n139-k10 | 13 590   | 85%               | 92%       | 81%       | 86%       | 70% (69)              | 13 599                | 26                    | 0.06%                 | -           | -           | -           | 13 846 | 1.88%  |
|          | X-n143-k07 | 15 722   | 81%               | 87%       | 88%       | 88%       | 74% (88)              | 15 726                | 54                    | 0.02%                 | -           | -           | -           | 16 245 | 3.32%  |
| Average  |            | 23 886   | 79%               | 85%       | 71%       | 78%       | 59% (50)              | 24 256                | 65                    | 0.22%                 | -           | -           | -           | 24 987 | 2.82%  |
| Medium   | X-n101-k25 | 27 606   | 83%               | 77%       | 63%       | 70%       | 56% (24)              | 27 736                | 18                    | 0.47%                 | 27 606      | 324         | 20.4        | 28 298 | 2.51%  |
|          | X-n106-k14 | 26 358   | 66%               | 95%       | 73%       | 84%       | 50% (34)              | 26 380                | 14                    | 0.08%                 | 26 358      | 355         | 23.9        | 26 871 | 1.95%  |
|          | X-n110-k13 | 14 971   | 87%               | 98%       | 67%       | 82%       | 59% (37)              | 14 993                | 12                    | 0.15%                 | 14 969      | 283         | 24.5        | 15 137 | 1.11%  |
|          | X-n125-k30 | 55 713   | 60%               | 74%       | 73%       | 74%       | 54% (25)              | 55 758                | 198                   | 0.08%                 | 55 655      | 5156        | 58.4        | 58 735 | 5.42%  |
|          | X-n129-k18 | 28 862   | 60%               | 74%       | 75%       | 75%       | 55% (48)              | 29 124                | 105                   | 0.91%                 | -           | -           | -           | 29 801 | 3.26%  |
|          | X-n134-k13 | 10 888   | 63%               | 74%       | 73%       | 74%       | 56% (41)              | 11 024                | 320                   | 1.25%                 | -           | -           | -           | 11 304 | 3.82%  |
|          | X-n139-k10 | 13 601   | 90%               | 85%       | 76%       | 81%       | 70% (64)              | 13 608                | 27                    | 0.05%                 | -           | -           | -           | 13 863 | 1.92%  |
|          | X-n143-k07 | 15 707   | 85%               | 97%       | 79%       | 88%       | 68% (79)              | 15 710                | 63                    | 0.02%                 | -           | -           | -           | 16 200 | 3.14%  |
| Average  |            | 24 213   | 74%               | 84%       | 72%       | 78%       | 57% (47)              | 24 292                | 95                    | 0.38%                 | -           | -           | -           | 25 026 | 2.89%  |
| Large    | X-n101-k25 | 27 651   | 70%               | 63%       | 77%       | 70%       | 65% (21)              | 28 125                | 122                   | 1.71%                 | 27 648      | 399         | 7.9         | 28 142 | 1.98%  |
|          | X-n106-k14 | 26 412   | 65%               | 84%       | 74%       | 79%       | 54% (32)              | 26 557                | 250                   | 0.54%                 | -           | -           | -           | 26 846 | 1.64%  |
|          | X-n110-k13 | 15 030   | 75%               | 86%       | 73%       | 80%       | 58% (34)              | 15 107                | 35                    | 0.51%                 | 15 030      | 2283        | 60.7        | 15 187 | 1.04%  |
|          | X-n125-k30 | 55 733   | 55%               | 81%       | 75%       | 78%       | 50% (23)              | 55 825                | 422                   | 0.16%                 | -           | -           | -           | 58 500 | 4.97%  |
|          | X-n129-k18 | 28 755   | 62%               | 78%       | 73%       | 75%       | 54% (45)              | 28 972                | 171                   | 0.76%                 | 28 748      | 2457        | 22.5        | 29 546 | 2.75%  |
|          | X-n134-k13 | 10 908   | 69%               | 72%       | 68%       | 70%       | 56% (38)              | 10 908                | 164                   | 0.66%                 | -           | -           | -           | 11 267 | 3.29%  |
|          | X-n139-k10 | 13 600   | 83%               | 85%       | 75%       | 80%       | 65% (59)              | 13 635                | 47                    | 0.26%                 | -           | -           | -           | 13 850 | 1.84%  |
|          | X-n143-k07 | 15 716   | 88%               | 97%       | 73%       | 85%       | 65% (77)              | 15 717                | 62                    | 0.00%                 | -           | -           | -           | 16 265 | 3.49%  |
| Average  |            | 24 226   | 71%               | 81%       | 73%       | 77%       | 57% (44)              | 24 365                | 159                   | 0.58%                 | -           | -           | -           | 24 950 | 2.62%  |

obtained by the FILO heuristic (over the 10 executions of the heuristic with different seeds). In the fourth column, we report the average similarity between the solution of the original instance and the solution of the modified instances computed by the following formula:

| | Notice that this similarity also matches the percentage of edges to fix (i.e., with the label 1). The next three columns summarize the performance of the ML model and show the following metrics: the true negative rate (TNR) corresponds to the percentage of edges that should not be fixed and that are predicted accurately; the true positive rate (TPR) is equal to the percentage of edges that should be fixed and that are predicted correctly; and the (balanced) accuracy is the mean of the two previous columns. By fixing the edges suggested by the ML model and solving the instance using VRPSolver, we obtain the results shown under the heading 'Edge-fixing'. The first column presents the percentage of fixed edges belonging to the original solution. The number of nodes removed in the network reduction step (see Subsection 4.2.3) is indicated in parenthesis. Next, we report the average costs of the solutions and the computing times in seconds. To evaluate the quality of the solutions, the fourth column shows the average gap, comparing the solution cost of our method with that of the FILO heuristic. The next three columns provide the average cost of the solutions computed by the exact algorithm in VRPSolver, the average

$$\sin ( S _ { o }, S _ { m } ) & = \frac { | E ( S _ { o } ) \cap E ( S _ { m } ) | } { | E ( S _ { o } ) | }. & ( 8 ) & = \frac { \frac { \alpha } { 8 } } { \frac { \beta } { \beta } } \\ \sin ( S _ { o } )$$

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

TABLE 4 Average results for scenarios with Nc = 20.

|          |            |          |                   | MLmodel metrics   | MLmodel metrics   | MLmodel metrics   | Edge-fixing   | Edge-fixing   | Edge-fixing   | Edge-fixing   | Exact B&P   | Exact B&P   | Exact B&P   | DACT   | DACT   |
|----------|------------|----------|-------------------|-------------------|-------------------|-------------------|---------------|---------------|---------------|---------------|-------------|-------------|-------------|--------|--------|
| Interval |  o        |  m cost | sim (  o ,  m ) | TNR               | TPR               | Accuracy          | %Reduc.       | Cost          | Time (s)      | Gap           | Cost        | Time (s)    | Ratio       | Cost   | Gap    |
| Small    | X-n101-k25 | 27 486   | 83%               | 73%               | 75%               | 74%               | 67% (25)      | 27 628        | 29            | 0.51%         | 27 486      | 231         | 8.6         | 28 220 | 2.67%  |
|          | X-n106-k14 | 26 338   | 72%               | 90%               | 71%               | 81%               | 54% (39)      | 26 401        | 30            | 0.24%         | 26 336      | 1147        | 88.5        | 26 845 | 1.93%  |
|          | X-n110-k13 | 14 980   | 77%               | 86%               | 74%               | 80%               | 60% (41)      | 15 024        | 9             | 0.29%         | 14 980      | 429         | 55.5        | 15 152 | 1.15%  |
|          | X-n125-k30 | 55 493   | 63%               | 85%               | 68%               | 77%               | 48% (25)      | 55 509        | 233           | 0.03%         | -           | -           | -           | 58 372 | 5.19%  |
|          | X-n129-k18 | 29 020   | 56%               | 71%               | 78%               | 74%               | 56% (44)      | 29 408        | 143           | 1.33%         | 29 009      | 5791        | 76.5        | 29 703 | 2.35%  |
|          | X-n134-k13 | 10 909   | 73%               | 85%               | 69%               | 77%               | 54% (43)      | 10 943        | 150           | 0.32%         | -           | -           | -           | 11 323 | 3.80%  |
|          | X-n139-k10 | 13 613   | 87%               | 91%               | 81%               | 86%               | 72% (71)      | 13 616        | 103           | 0.02%         | -           | -           | -           | 13 870 | 1.89%  |
|          | X-n143-k07 | 15 715   | 86%               | 90%               | 86%               | 88%               | 75% (86)      | 15 748        | 30            | 0.21%         | -           | -           | -           | 16 261 | 3.47%  |
| Average  |            | 24 194   | 75%               | 84%               | 75%               | 80%               | 60% (47)      | 24 284        | 91            | 0.37%         | -           | -           | -           | 24 968 | 2.81%  |
| Medium   | X-n101-k25 | 27 512   | 68%               | 74%               | 72%               | 73%               | 57% (22)      | 27 694        | 32            | 0.66%         | 27 512      | 518         | 19.4        | 28 076 | 2.05%  |
|          | X-n106-k14 | 26 306   | 64%               | 87%               | 71%               | 79%               | 50% (38)      | 26 400        | 20            | 0.36%         | 26 306      | 5257        | 238.2       | 26 785 | 1.72%  |
|          | X-n110-k13 | 14 983   | 73%               | 86%               | 77%               | 82%               | 60% (37)      | 15 078        | 27            | 0.64%         | 14 983      | 548         | 30.3        | 15 217 | 1.56%  |
|          | X-n125-k30 | 55 503   | 54%               | 82%               | 74%               | 78%               | 48% (22)      | 55 579        | 233           | 0.14%         | -           | -           | -           | 58 528 | 5.45%  |
|          | X-n129-k18 | 29 171   | 57%               | 73%               | 76%               | 74%               | 55% (38)      | 29 655        | 315           | 1.66%         | -           | -           | -           | 30 001 | 2.85%  |
|          | X-n134-k13 | 10 877   | 55%               | 82%               | 80%               | 81%               | 52% (41)      | 10 956        | 127           | 0.73%         | -           | -           | -           | 11 258 | 3.50%  |
|          | X-n139-k10 | 13 605   | 72%               | 87%               | 80%               | 84%               | 61% (64)      | 13 631        | 205           | 0.20%         | -           | -           | -           | 13 855 | 1.84%  |
|          | X-n143-k07 | 15 708   | 81%               | 80%               | 83%               | 82%               | 71% (85)      | 15 745        | 49            | 0.24%         | -           | -           | -           | 16 233 | 3.35%  |
| Average  |            | 24 208   | 65%               | 81%               | 77%               | 79%               | 57% (43)      | 24 342        | 126           | 0.58%         | -           | -           | -           | 24 991 | 2.79%  |
| Large    | X-n101-k25 | 27 645   | 59%               | 70%               | 73%               | 72%               | 55% (21)      | 27 871        | 56            | 0.81%         | 27 645      | 466         | 30.8        | 28 244 | 2.17%  |
|          | X-n106-k14 | 26 413   | 61%               | 79%               | 74%               | 77%               | 53% (37)      | 26 555        | 100           | 0.54%         | -           | -           | -           | 26 834 | 1.59%  |
|          | X-n110-k13 | 15 034   | 68%               | 81%               | 79%               | 80%               | 60% (34)      | 15 161        | 23            | 0.84%         | 15 034      | 487         | 25.4        | 15 216 | 1.21%  |
|          | X-n125-k30 | 55 958   | 58%               | 79%               | 70%               | 74%               | 49% (21)      | 56 168        | 433           | 0.37%         | -           | -           | -           | 58 629 | 4.77%  |
|          | X-n129-k18 | 29 123   | 55%               | 77%               | 74%               | 75%               | 51% (34)      | 29 462        | 386           | 1.16%         | 29 092      | 3106        | 16.0        | 29 848 | 2.49%  |
|          | X-n134-k13 | 10 909   | 55%               | 82%               | 80%               | 81%               | 52% (40)      | 11 051        | 254           | 1.31%         | -           | -           | -           | 11 317 | 3.75%  |
|          | X-n139-k10 | 13 585   | 73%               | 88%               | 77%               | 83%               | 59% (61)      | 13 619        | 139           | 0.25%         | -           | -           | -           | 13 788 | 1.50%  |
|          | X-n143-k07 | 15 743   | 84%               | 79%               | 79%               | 79%               | 70% (84)      | 15 775        | 41            | 0.20%         | -           | -           | -           | 16 332 | 3.74%  |
| Average  |            | 24 301   | 64%               | 79%               | 76%               | 78%               | 56% (42)      | 24 458        | 179           | 0.69%         | -           | -           | -           | 25 026 | 2.65%  |

computing time and the ratio with respect to the computing time of our method (i.e., the computing time of the exact B&amp;P algorithm divided by the computing time of our edge-fixing method). For the exact algorithm, a time limit of 5 h is set. Empty values mean that the exact B&amp;P failed to find an optimal solution in the time limit for one or more of the five instances, refer to Appendix A for additional details.

Anatural question with respect to the proposed approach is if there is value in trying to learn the difference between solutions of slightly modified instances. Indeed, ML-based CVRP heuristics should be in a very favorable position in our computational setting because the instances are from a specific data distribution and of the same size, that is, no generalization is needed. In order to give a tentative answer to this question, we compare the performance of our method with another ML method for the CVRP not designed for reoptimization purposes but with excellent overall performance on recent benchmarks. We chose the Dual-Aspect Collaborative Transformer algorithm (DACT; [22]) that is considered one of the most effective methods for solving the CVRP to-date in terms of solution quality according to the recent survey [6]. Unlike our approach, DACT is a reinforcement learning method that learns an improvement heuristic, which means that it starts with an initial solution and tries to improve it in an iterative way. In this method, the learner (i.e., the agent) learns to identify a pair of nodes on which to apply a pairwise operator (e.g., 2-opt, swap, insert) and is rewarded when a better solution is found. The authors report good results that outperform several other learning methods on the X benchmark instances. In our case, given that reinforcement learning models require a large volume of training data, we proceeded by using the pre-trained model of [22] but with additional training on the eight instances we focus on (see Table 1A), in addition to using the same parameters of their best performing model. The results obtained are reported in the last two columns of Tables 3-5, representing respectively the average cost of the solutions and the gap with respect to the FILO heuristic (just like Edge-Fixing).

According to the results reported in Tables 3-5, we can notice that on average the similarity decreases by increasing the number of changes we make on the demands (controlled by the parameter Nc and the intervals), which is expected. A high similarity of 79% is noticed on average on the instances with 10% change and small interval, while the instances with 30% change and large interval have an average similarity of 63%. At the instance level, it seems that the dispersion of the clients as well as the length of the routes can have an impact on the similarity of the solutions. For example, the instances with medium to long routes (e.g., X-n110-k13, X-n139-k10, X-n143-k07) tend to have a higher similarity compared to instances with shorter routes (e.g., X-n129-k18 and X-n125-k30).

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

TABLE 5 Average results for scenarios with Nc = 30.

| Interval   |            |        | cost sim (  o ,    | MLmodel metrics   | MLmodel metrics   | MLmodel metrics   | Edge-fixing   | Edge-fixing   | Edge-fixing   | Edge-fixing   | Exact B&P   | Exact B&P   | Exact B&P   | DACT   | DACT   |
|------------|------------|--------|----------------------|-------------------|-------------------|-------------------|---------------|---------------|---------------|---------------|-------------|-------------|-------------|--------|--------|
| Interval   |  o        |  m    | cost sim (  o ,    | TNR               | TPR               | Accuracy          | %Reduc.       | Cost          | Time (s)      | Gap           | Cost        | Time (s)    | Ratio       | Cost   | Gap    |
| Small      | X-n101-k25 | 27 562 | 75%                  | 80%               | 70%               | 75%               | 58% (22)      | 27 669        | 37            | 0.39%         | 27 562      | 178         | 5.9         | 28 219 | 2.38%  |
|            | X-n106-k14 | 26 383 | 65%                  | 89%               | 69%               | 79%               | 49% (38)      | 26 438        | 148           | 0.21%         | 26 378      | 1577        | 25.9        | 26 833 | 1.70%  |
|            | X-n110-k13 | 15 005 | 74%                  | 92%               | 77%               | 84%               | 58% (38)      | 15 083        | 32            | 0.52%         | 15 005      | 555         | 21.8        | 15 157 | 1.01%  |
|            | X-n125-k30 | 55 776 | 53%                  | 77%               | 73%               | 75%               | 50% (24)      | 55 834        | 259           | 0.10%         | -           | -           | -           | 58 601 | 5.06%  |
|            | X-n129-k18 | 29 414 | 58%                  | 80%               | 74%               | 77%               | 51% (42)      | 29 778        | 237           | 1.24%         | 29 405      | 3742        | 24.8        | 30 086 | 2.28%  |
|            | X-n134-k13 | 10 925 | 77%                  | 90%               | 62%               | 76%               | 50% (39)      | 10 952        | 140           | 0.25%         | -           | -           | -           | 11 361 | 4.00%  |
|            | X-n139-k10 | 13 622 | 83%                  | 91%               | 73%               | 82%               | 62% (57)      | 13 692        | 154           | 0.51%         | -           | -           | -           | 13 829 | 1.52%  |
|            | X-n143-k07 | 15 754 | 83%                  | 79%               | 87%               | 83%               | 76% (84)      | 15 822        | 56            | 0.43%         | -           | -           | -           | 16 328 | 3.65%  |
| Average    |            | 24 305 | 71%                  | 85%               | 73%               | 79%               | 56% (43)      | 24 409        | 133           | 0.46%         | -           | -           | -           | 25 052 | 2.70%  |
| Medium     | X-n101-k25 | 27 654 | 63%                  | 68%               | 73%               | 71%               | 58% (20)      | 27 804        | 51            | 0.54%         | 27 651      | 243         | 24.8        | 28 277 | 2.25%  |
| Medium     | X-n106-k14 | 26 437 | 59%                  | 75%               | 70%               | 73%               | 52% (38)      | 26 615        | 247           | 0.67%         | -           | -           | -           | 26 780 | 1.30%  |
| Medium     | X-n110-k13 | 15 058 | 77%                  | 87%               | 72%               | 79%               | 58% (36)      | 15 095        | 14            | 0.24%         | 15 058      | 269         | 33.6        | 15 292 | 1.55%  |
| Medium     | X-n125-k30 | 55 925 | 50%                  | 82%               | 78%               | 80%               | 48% (22)      | 56 009        | 451           | 0.15%         | -           | -           | -           | 58 859 | 5.25%  |
|            | X-n129-k18 | 29 221 | 54%                  | 79%               | 75%               | 77%               | 50% (40)      | 29 698        | 137           | 1.63%         | 29 215      | 3574        | 34.8        | 29 849 | 2.16%  |
|            | X-n134-k13 | 10 926 | 56%                  | 83%               | 80%               | 81%               | 52% (38)      | 11 048        | 129           | 1.12%         | -           | -           | -           | 11 301 | 3.43%  |
|            | X-n139-k10 | 13 640 | 75%                  | 89%               | 78%               | 83%               | 61% (56)      | 13 708        | 256           | 0.49%         | -           | -           | -           | 13 926 | 2.09%  |
|            | X-n143-k07 | 15 782 | 80%                  | 78%               | 85%               | 82%               | 72% (82)      | 15 870        | 49            | 0.55%         | -           | -           | -           | 16 423 | 4.06%  |
| Average    |            | 24 330 | 64%                  | 80%               | 76%               | 78%               | 56% (41)      | 24 481        | 167           | 0.67%         | -           | -           | -           | 25 088 | 2.76%  |
| Large      | X-n101-k25 | 27 617 | 66%                  | 76%               | 72%               | 74%               | 56% (20)      | 27 929        | 185           | 1.12%         | 27 617      | 339         | 19.4        | 28 207 | 2.13%  |
|            | X-n106-k14 | 26 440 | 54%                  | 79%               | 68%               | 74%               | 46% (36)      | 26 637        | 275           | 0.75%         | -           | -           | -           | 26 913 | 1.79%  |
|            | X-n110-k13 | 15 090 | 66%                  | 82%               | 72%               | 77%               | 54% (37)      | 15 211        | 88            | 0.80%         | 15 090      | 1120        | 18.7        | 15 318 | 1.51%  |
|            | X-n125-k30 | 56 228 | 52%                  | 84%               | 71%               | 78%               | 45% (22)      | 56 221        | 475           | - 0 . 01%     | -           | -           | -           | 58 725 | 4.44%  |
|            | X-n129-k18 | 29 674 | 58%                  | 75%               | 73%               | 74%               | 53% (40)      | 30 179        | 75            | 1.69%         | 29 670      | 4914        | 69.9        | 30 554 | 2.97%  |
|            | X-n134-k13 | 10 950 | 53%                  | 83%               | 82%               | 82%               | 51% (36)      | 11 029        | 145           | 0.72%         | -           | -           | -           | 11 423 | 4.31%  |
|            | X-n139-k10 | 13 616 | 73%                  | 91%               | 76%               | 84%               | 58% (56)      | 13 635        | 128           | 0.14%         | -           | -           | -           | 13 868 | 1.85%  |
|            | X-n143-k07 | 15 884 | 79%                  | 82%               | 83%               | 83%               | 69% (80)      | 15 941        | 313           | 0.36%         | -           | -           | -           | 16 326 | 2.79%  |
| Average    |            | 24 437 | 63%                  | 82%               | 75%               | 78%               | 54% (41)      | 24 598        | 211           | 0.70%         | -           | -           | -           | 25 167 | 2.72%  |

Before evaluating the performance of the model, we would like to highlight the impact that a bad prediction may have in the obtained solution. If some edges are not fixed when they should be (false negatives), it can affect the computing time but the solver can still include them in the final solution. However, the edges with label 0 can have a more serious impact, since they can affect the quality of the solution if predicted inaccurately (false positives), which potentially leads to a higher gap. As previously mentioned, the similarity is also the percentage of edges to be fixed. A high similarity means that most of the edges in the solution of the original instance are labeled 1, which also implies that the ML model tends to make fewer 'significant' errors since there are not many edges with label 0. If we focus on the solution quality, it is possible to give a higher weight to the edges with label 0, which can lead to a high TNR (thus reducing the false positives) and probably a lower TPR, meaning that less edges will be fixed resulting in a higher computing time. Conversely, if we do the opposite and assign a higher weight to the edges with label 1, we will likely fix more edges and achieve a lower computing time but at the expense of bad-quality solutions. Depending on the desired goal, one seeks to find a compromise between quality and computing time. In our case, we use a balanced weight between the two classes as shown in Table 2, resulting in a number of fixed edges from the original solution varying between 54% and 60% on average and of nodes removed varying between 41 and 50 on average. As discussed below, these network reductions yield a substantial speedup without deteriorating solution quality too much.

From the ML metrics obtained we can observe an average accuracy ranging from 78 % to 80 % , but if we look closer at the individual instances, we can notice some variance and a slight positive correlation between the accuracy and the similarity. By fixing the edges proposed by the model and comparing the solution with the one of the FILO heuristic, we obtain an average gap ranging from 0 22 . % to 0 70 . % . We can also notice that the gap follows the same tendency as the similarity, that is, more changes of the demands imply less similarity, less edges to fix, more possible misclassifications and thus a higher gap. Therefore, the instances with high similarity generally have a lower gap compared to the others. The detailed results in Appendix A indicate that the gap of the individual instances can vary between 0 % and 1 71 . % . A perfect solution is obtained (i.e., same solution as the FILO heuristic, thus a 0 % gap) when the TNR is 100 % . In a few rare cases, the solver was able to find a better solution than the heuristic even after fixing parts of the solution, such as X-n125-k30\_10M\_4 in Table A2 and X-n125-k30\_30L\_3 in Table A9. Generally, the FILO heuristic finds very good solutions (especially when considering 10 runs): it succeeds in finding the optimal solution in many cases when compared to the available exact B&amp;P results. We believe that the gap reported between the edge-fixing and the FILO heuristic must not be far from the one with the exact B&amp;P algorithm.

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

![Image](image_000002_55466050a2932e54e16dfcc8372e7716829bc6bff46d877700dde7cb7903b87c.png)

In terms of computing time, the edge fixing method takes only a few minutes or even a few seconds in some cases to find the optimal solution of the modified problem (that with fixed variables according to the ML prediction), whereas the exact B&amp;P algorithm that solves the (original) problem from scratch can take hours of computation to find an optimal solution, and in many cases no solution is found after the time limit especially for large instances. Apparently, the current version of VRPSolver does not have strong heuristics to produce feasible solutions. On the one side, this is not an issue for our computational investigation because we use VRPSolver as well (and we are definitely not using it for comparison). On the other side, this is somehow reinforcing the fact that, after our proposed fixing, the instances are much easier and even without good heuristics VRPSolver is very effective on them. Note that in our experiments, the FILO heuristic with 1 000 000 iterations takes about 8 to 13 min per execution depending on the instance size, not to forget that we executed the heuristic 10 different times with different seeds and retained the best solution. Overall, the computing time of the edge-fixing heuristic remains lower than a single execution of FILO and way lower than an exact B&amp;P algorithm.

Concerning the DACT method, we can notice an average gap of 2 7 . % . The method performs better on some instances than on others with gaps ranging from 0 03 . % up to 5 72 . % according to the detailed results in Appendix A. Since the method is iterative and performs a certain number of steps at the inference phase (more precisely 10 000 steps as described in [22]), its computing time is not negligible and can range from 10 to 15 min on average depending on the instance size. Therefore, the edge-fixing method outperforms DACT both in terms of computing time and solution quality. This shows that there is benefit in learning to re-optimize compared to learning to solve the problem directly.

## 6 CONCLUSION

In this paper, we proposed a ML-based heuristic for the reoptimization of repeatedly solved routing problems after minor changes in the problem data. More precisely, we put ourselves in the context of a company that solves CVRPs on a daily basis where the locations of the clients are the same but slightly different demands occur. Given that there can be a great similarity between the solutions, the goal was to exploit the ones obtained from previous executions (not necessarily optimal ones) in order to speed up the reoptimization of future instances. The aim was to predict and fix the edges that have a high chance of remaining in the solution after a change of the demands.

Following a supervised learning approach, we trained neural network models on the data collected from a recent heuristic for the CVRP called FILO. The models achieved an average accuracy of 78% on the test instances. By incorporating the predictions in our edge-fixing method, we were able to find solutions in reasonable computing times with gaps ranging from 0 % to 1 71 . % (with an average of 0 51 . % ) when compared to the FILO heuristic we learned from. These gaps are also lower compared to other recent ML methods proposed in the literature such as the DACT method used to compute the entire CVRP solution. We also obtained an acceleration effect, considerably reducing the size of the network and allowing an even faster reoptimization.

Future work can seek some improvements, for example by the exploration of more complex learning models that can take advantage of the graph structure of the problem or even sequence models since we are dealing with routes. This may potentially lead to a better accuracy and therefore better gaps and lower computing times.

Amajor limitation of our approach in its present form is that we do not characterize a priori the concept of 'slight' modifications to the data but we are only able to observe a posteriori if such modifications lead to similar solutions between the original instance and the modified one. By experience, such observation is often not difficult to make, like for example in the CVRP context we considered. However, future research should be devoted to study such characterization so as to provide guidelines on the suitability of the approach to new problems.

## ACKNOWLEDGMENTS

We are indebted to two anonymous referees whose detailed reading helped significantly improve the paper.

## DATA AVAILABILITY STATEMENT

The data that support the findings of this study are openly available in CVRPLIB at http://vrp.atd-lab.inf.puc-rio.br/ index.php/en/.

## ORCID

Andrea Lodi https://orcid.org/0000-0001-9269-633X

## REFERENCES

- [1] L. Accorsi and D. Vigo, Afast and scalable heuristic for the solution of large-scale capacitated vehicle routing problems , Transp. Sci. 55 (2021), no. 4, 832-856. https://doi.org/10.1287/trsc.2021.1059.

- [2] A. M. Alvarez, Q. Louveaux, and L. Wehenkel, Amachine learning-based approximation of strong branching , INFORMS J. Comput. 29 (2017), no. 1, 185-195. https://doi.org/10.1287/ijoc.2016.0723.
- [3] C. Barnhart, E. L. Johnson, G. L. Nemhauser, M. W. P. Savelsbergh, and P. H. Vance, Branch-and-price: Column generation for solving huge integer programs , Oper. Res. 46 (1996), 316-329.
- [4] I. Bello, H. Pham, Q. V. Le, M. Norouzi, and S. Bengio. Neural combinatorial optimization with reinforcement learning. 2016 https://arxiv.org /abs/1611.09940.
- [5] Y. Bengio, A. Lodi, and A. Prouvost, Machine learning for combinatorial optimization: A methodological tour d'horizon , Eur. J. Oper. Res. 290 (2021), no. 2, 405-421. https://doi.org/10.1016/j.ejor.2020.07.063.
- [6] A. Bogyrbayeva, M. Meraliyev, T. Mustakhov, and B. Dauletbayev. Learning to solve vehicle routing problems: A survey. 2022 https://arxiv .org/abs/2205.02453.
- [7] P. Bonami, A. Lodi, and G. Zarpellon, A classifier to decide on the linearization of mixed-integer quadratic problems in cplex , Oper. Res. 70 (2022), 3303-3320.
- [8] Q. Cappart, D. Chételat, E. Khalil, A. Lodi, C. Morris, and P. Veliˇkovi´ c. Combinatorial optimization and reasoning with graph neural networks. c arXiv preprint arXiv:2102.09544. 2021.
- [9] G. Clarke and J. W. Wright, Scheduling of vehicles from a central depot to a number of delivery points , Oper. Res. 12 (1964), no. 4, 568-581.
- [10] L. Costa, C. Contardo, and G. Desaulniers, Exact branch-price-and-cut algorithms for vehicle routing , Transp. Sci. 53 (2019), no. 4, 946-985. https://doi.org/10.1287/trsc.2018.0878.
- [11] H. Dai, E. B. Khalil, Y. Zhang, B. Dilkina, and L. Song. Learning combinatorial optimization algorithms over graphs. 2017 https://arxiv.org /abs/1704.01665.
- [12] G. Desaulniers, J. Desrosiers, and M. M. Solomon, Column generation , Springer, New York, 2005.
- [13] M. Gasse, D. Chételat, N. Ferroni, L. Charlin, and A. Lodi. Exact combinatorial optimization with graph convolutional neural networks. 2019 https://arxiv.org/abs/1906.01629.
- [14] S. Irnich and G. Desaulniers, ' Shortest path problems with resource constraints ,' Column generation , G. Desaulniers, J. Desrosiers, and M. M. Solomon (eds.), Springer US, Boston, MA, 2005, pp. 33-65.
- [15] C. K. Joshi, T. Laurent, and X. Bresson. An efficient graph convolutional network technique for the travelling salesman problem. 2019 https:/ /arxiv.org/abs/1906.01227.
- [16] E. B. Khalil, P. Le Bodic, L. Song, G. Nemhauser, and B. Dilkina. Learning to branch in mixed integer programming. Proceedings of the thirtieth AAAI conference on artificial intelligence, AAAI'16, page 724-731. AAAI Press. 2016.
- [17] W. Kool, H. van Hoof, and M. Welling. Attention, learn to solve routing problems! 2018 https://arxiv.org/abs/1803.08475.
- [18] J. Kotary, F. Fioretto, P. Van Hentenryck, and B. Wilder. End-to-end constrained optimization learning: A survey. 2021 https://arxiv.org/abs /2103.16378.
- [19] M. Kruber, M. E. Lübbecke, and A. Parmentier, ' Learning when to use a decomposition ,' Integration of AI and OR techniques in constraint programming , D. Salvagnin and M. Lombardi (eds.), Springer International Publishing, Cham, 2017, pp. 202-210.
- [20] Z. Li, Q. Chen, and V. Koltun. Combinatorial optimization with graph convolutional networks and guided tree search. 2018 https://arxiv.org /abs/1810.10659.
- [21] A. Lodi, L. Mossina, and E. Rachelson, Learning to handle parameter perturbations in combinatorial optimization: An application to facility location , EURO J Transp Logist 9 (2020), no. 4, 100023. https://doi.org/10.1016/j.ejtl.2020.100023.
- [22] Y. Ma, J. Li, Z. Cao, W. Song, L. Zhang, Z. Chen, and J. Tang. Learning to iteratively solve routing problems with dual-aspect collaborative transformer. 2021 https://arxiv.org/abs/2110.02544.
- [23] S. Manchanda, A. Mittal, A. Dhawan, S. Medya, S. Ranu, and A. Singh. Learning heuristics over large graphs via deep reinforcement learning. 2019 https://arxiv.org/abs/1903.03332.
- [24] M. R. Nazari, A. Oroojlooy, L. Snyder, and M. Takac, ' Reinforcement learning for solving the vehicle routing problem ,' Advances in neural information processing systems , Vol 31 , S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (eds.), Curran Associates, Inc, New York, 2018.
- [25] A. Pessoa, R. Sadykov, E. Uchoa, and F. Vanderbeck, A generic exact solver for vehicle routing and related problems , Math. Program. 183 (2020), no. 1, 483-523. https://doi.org/10.1007/s10107-020-01523-z.
- [26] E. Uchoa, D. Pecin, A. Pessoa, M. Poggi, T. Vidal, and A. Subramanian, New benchmark instances for the capacitated vehicle routing problem , Eur. J. Oper. Res. 257 (2017), no. 3, 845-858. https://doi.org/10.1016/j.ejor.2016.08.012.
- [27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all you need. 2017 https:/ /arxiv.org/abs/1706.03762.
- [28] Á. S. Xavier, F. Qiu, and S. Ahmed, Learning to solve large-scale security-constrained unit commitment problems , INFORMS J. Comput. 33 (2021), no. 2, 739-756. https://doi.org/10.1287/ijoc.2020.0976.
- [29] G. Zarpellon, J. Jo, A. Lodi, and Y. Bengio. Parameterizing branch-and-bound search trees to learn branching policies. 2020 https://arxiv.org /abs/2002.05120.

,

How to cite this article: M. Morabit, G. Desaulniers, and A. Lodi, Learning to repeatedly solve routing problems Networks. 83 (2024), 503-526. https://doi.org/10.1002/net.22200

## APPENDIX A: DETAILED RESULTS OF THE EXPERIMENTS

This section provides detailed computational results that are complementary to those presented in Section 5. The results are grouped by scenario ( Nc and interval) and reported in Tables A1-A9. The same information as in Tables 3-5 is reported, with the only difference that each row represents an individual instance. Also, an additional column has been added to indicate the name suffix of the modified instance  m (second column).

## TABLE A1 Results for instances with Nc = 10 and small (S) intervals.

![Image](image_000003_9a4c45558b9985c75f07b8b4667fe65fedfafcae007e3c22af7e86cb169be4e6.png)

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## TABLE A2 Results for instances with Nc = 10 and medium (M) intervals.

![Image](image_000004_40e047913e91355c3b8b676688d9d707a6843410d598f85a194f042515e1ad43.png)

| Gap                          | 2.51% 1.88% 2.67% 2.28% 3.20% 2.51% 1.83% 2.11% 1.95% 1.57% 2.27% 1.95% 0.49% 1.10% 1.13% 1.94% 0.91% 1.11% 5.56% 5.71% 4.72% 5.03% 6.11% 5.42% 3.74% 3.80% 2.93% 2.87% 2.93% 3.26% 3.43% 3.97% 4.28% 3.43% 3.98% 3.82% 1.05% 3.10% 1.24% 2.08% 2.13% 1.92% 2.72% 3.96% 3.08% 3.20%   |                                              |                                                                         |                                                                                              |                                                                              | 2.74% 3.14%                                           |
|------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|------------------------------------------------------------------------------|-------------------------------------------------------|
| DACT Cost 28 417             | 28 156 28 143 28 286 28 489 28 298 26 882 26 898 26 853 26 796 59 103 58 735 29 734                                                                                                                                                                                                   | 26 928 26 871 14 972                         | 15 197 15 187 15 223 15 107 15 137 58 614 58 916 58 461 58 579          | 30 139 30 142 29 751 29 240 29 801 11 265 11 387 11 254 11 265 11 349                        | 11 304 13 727 14 027 13 775 13 894 13 890 13 863                             | 16 151 16 370 16 150 16 193 16 136 16 200             |
| Ratio 15.1                   | 12.9 19.5 11.7 42.9 20.4 36.0 18.4 15.0 32.6 17.6 11.5 58.4 5.3                                                                                                                                                                                                                       | 23.9 33.9 35.9                               | 7.1 11.9 33.9 24.5 28.9 99.7 115.8 36.3                                 | 141.4 - 89.1 4.5 - - - - -                                                                   | - - - - - - - -                                                              | - - - - - -                                           |
| Time (s) 544                 | 246 370 117 343 324 684 368 210 424 88 5156 356                                                                                                                                                                                                                                       | 355 441 502 106                              | 131 237 283 2284 7479 6486 834 8696                                     | 3817 > 5 h 2674 1290 - > 5 h > 5 h > 5 h > 5 h > 5 h                                         | - > 5 h > 5 h > 5 h > 5 h > 5 h                                              | - > 5 h > 5 h > 5 h > 5 h > 5 h -                     |
| Exact BaP Cost               | 27 722 27 636 27 411 27 655 27 605 27 606 26 399 26 341 26 340 26 381 55 650 55 655 28 661                                                                                                                                                                                            | 26 330 26 358 14 889 15 032                  | 15 018 14 933 14 971 14 969 55 529 55 620 55 828 55 647                 | 29 031 29 460 28 910 28 395 - - - - -                                                        | - - 13 584 - 13 649 - -                                                      | - - - - - - -                                         |
| Gap 0.89%                    | 0.74% 0.26% 0.47% 0.00% 0.47% 0.13% 0.11% 0.13% 0.04% 0.23% 0.08% 2.34%                                                                                                                                                                                                               | 0.00% 0.08% 0.07% 0.66%                      | 0.00% 0.00% 0.00% 0.15% 0.14% 0.13% 0.05% - 0 . 15%                     | 0.58% 0.94% 0.04% 0.65% 0.91% 1.76% 1.31% 1.09% 0.81% 1.28% 1.25%                            | 0.00% 0.12% 0.03% 0.00% 0.10% 0.05%                                          | 0.10% 0.00% 0.00% 0.00% 0.00% 0.02%                   |
| Time (s) 36                  | 19 19 10 8 18 19 20 14 13 755 198 67                                                                                                                                                                                                                                                  | 5 14 13 14 15                                | 11 7 12 79 75 56 23                                                     | 27 115 30 286 105 554 645 20 165 214                                                         | 320 48 18 28 12                                                              | 28 27 156 41 43 41 36 63                              |
| Cost 27 969                  | 27 841 27 482 27 784 27 605 27 736 26 433 26 370 26 374 26 392                                                                                                                                                                                                                        | 26 330 26 380 14 910 15 131 15 018           | 14 933 14 971 14 993 55 608 55 807 55 858 55 690 55 828 55 758 29 332   | 29 203 29 559 28 931 28 594 29 124 11 083 11 095 10 910 10 979 11 055                        | 11 024 13 584 13 621 13 610 13 611 13 613 13 608                             | 15 739 15 747 15 667 15 691 15 706 15 710             |
| Edge-fixing %Reduc. 57% (17) | 57% (23) 58% (25) 57% (25) 57% (29) 56% (24) 49% (28) 47% (34) 51% (34) 50% (33) 54% (25) 54% (50)                                                                                                                                                                                    | 48% (40) 51% (34) 57% (29) 56% (37) 58% (40) | 58% (38) 60% (39) 59% (37) 54% (22) 52% (26) 55% (28) 54% (23) 52% (26) | 57% (43) 54% (50) 56% (46) 55% (49) 55% (48) 54% (37) 54% (42) 54% (40) 53% (41) 54% (43)    | 56% (41) 69% (58) 70% (64) 71% (66) 70% (65) 70% (66) 70% (64)               | 69% (67) 70% (74) 66% (82) 65% (78) 68% (78) 68% (76) |
| Accuracy 73%                 | 69% 64% 63% 80% 70% 85% 87% 87% 86% 74% 75%                                                                                                                                                                                                                                           | 78% 84% 82% 82% 82%                          | 84% 82% 82% 74% 82% 65% 75% 73%                                         | 72% 73% 78% 76% 75% 78% 58% 74% 78% 81%                                                      | 74% 86% 86% 78% 89% 66% 81%                                                  | 90% 90% 90% 88% 84% 88%                               |
| MLmodel metrics TPR 63%      | 66% 61% 65% 59% 63% 79% 76% 78% 78% 78% 71% 73% 77%                                                                                                                                                                                                                                   | 55% 73% 69% 69%                              | 64% 68% 63% 67% 68% 82% 64%                                             | 74% 73% 76% 75% 75% 77% 55% 76% 77%                                                          | 81% 73% 71% 78% 82% 77% 72% 76%                                              | 84% 80% 80% 75% 78% 79%                               |
| o ,  m ) TNR 83%            | 72% 67% 61% 100% 77% 90% 97% 96% 93% 100% 72% 74% 74% 72%                                                                                                                                                                                                                             | 95% 95% 95%                                  | 100% 100% 100% 98% 79% 82% 65%                                          | 70% 73% 79% 77% 74% 79% 60% 71% 79%                                                          | 80% 74% 100% 93% 74% 100% 60%                                                | 85% 96% 100% 100% 100% 90% 97%                        |
| sim (  88%                  | 76% 88% 69% 96% 83% 56% 63% 61% 60% 88% 51% 57% 60% 54%                                                                                                                                                                                                                               | 66% 82% 82% 91%                              | 86% 95% 87% 70% 53% 68%                                                 | 61% 58% 64% 62% 60% 59% 90% 54%                                                              | 57% 55% 63% 97% 89% 81% 91% 93%                                              | 90% 81% 87% 83% 87% 85% 85%                           |
|  m cost 27 722              | _10M_2 27 636 _10M_3 27 411 27 655 27 605 27 606 26 399 26 26 26 55 773 55 700 55 713 28 661                                                                                                                                                                                          | 341 340 381 26 330 26 358 14 899 15 032 018  | 15 14 933 14 971 14 971 55 529 55 735 55 828                            | 29 035 29 283 _10M_4 28 920 _10M_5 28 409 - 28 862 _10M_1 10 891 _10M_2 _10M_3 _10M_4 _10M_5 | 10 952 10 792 10 891 10 915 10 888 13 584 13 605 13 606 13 611 13 600 13 601 | 15 724 15 747 15 667 15 691 15 706 15 707             |
|  m _10M_1                   | _10M_4 _10M_5 - _10M_1 _10M_2 _10M_3 _10M_4 _10M_4 _10M_5 - _10M_1                                                                                                                                                                                                                    | _10M_5 - _10M_1 _10M_2 _10M_3                | _10M_4 _10M_5 - _10M_1 _10M_2 _10M_3                                    | _10M_2 _10M_3                                                                                | - _10M_1 _10M_2 _10M_3 _10M_4 _10M_5                                         | - _10M_1 _10M_2 _10M_3 _10M_4 _10M_5 -                |
| o                            | Average X-n106-k14 Average X-n129-k18                                                                                                                                                                                                                                                 |                                              |                                                                         |                                                                                              |                                                                              | X-n143-k07 Average                                    |
|  X-n101-k25                 |                                                                                                                                                                                                                                                                                       | Average X-n110-k13                           | Average X-n125-k30                                                      | Average X-n134-k13                                                                           | Average X-n139-k10 Average                                                   |                                                       |

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

TABLE A3 Results for instances with Nc = 10 and large (L) intervals.

![Image](image_000005_48d3dfd9161d4f6edc714468df339170e4746ec1bc6bf8d52cc00744a5bd5ad4.png)

| Gap                          | 2.30% 1.16% 2.94% 1.70% 1.78% 1.98% 1.42% 1.78% 1.92% 1.73% 1.35% 1.64% 0.03% 0.72% 1.18% 2.55% 0.73% 1.04% 4.97% 4.48% 4.04% 5.31% 6.02% 4.97% 2.83% 2.46% 3.27% 2.68% 2.52% 2.75% 2.89% 3.22% 3.12%   |                                              |                                                                       |                                                                | 3.23% 3.98% 3.29%                                     | 2.05% 2.08% 1.75% 1.52% 1.79% 1.84%                   | 3.81% 2.94% 2.85% 2.02% 5.83% 3.49%                   |
|------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------|-----------------------------------------------------------------------|----------------------------------------------------------------|-------------------------------------------------------|-------------------------------------------------------|-------------------------------------------------------|
| DACT Cost                    | 28 420 27 812 28 297 28 248 28 210 28 142 26 776 26 934 26 860 26 974 15 169                                                                                                                            | 26 686 26 846 14 950 15 313                  | 15 473 15 032 15 187 58 240 57 890 58 517 58 796 59 059 58 500 29 354 | 29 824 29 992 29 441 29 120 29 546                             | 11 251 11 293 11 129 11 297 11 363 11 267             | 13 859 13 870 13 833 13 832 13 855 13 850             | 16 335 16 196 16 175 16 020 16 598 16 265             |
| Ratio                        | 1.4 10.3 16.3 1.9 9.6 7.9 20.6 25.2 4.5                                                                                                                                                                 | - 10.6 - 58.8 107.5                          | 10.9 70.9 55.5 60.7 13.2 473.1 21.5 50.7 -                            | - 17.4 73.0 8.9 11.3 1.8 22.5                                  | - - - - - -                                           | 180.6 - 296.6 349.3 - -                               | - - - - - -                                           |
| Time (s) 123                 | 154 245 788 683 399 454 10 289 662 > 5 h 95                                                                                                                                                             | - 529 8812 676                               | 1063 333 2283 2091 17 979 11 651 5987 > 5 h                           | - 2873 5039 889 3035 451 2457                                  | > 5 h > 5 h > 5 h > 5 h > 5 h -                       | 13 184 > 5 h 7712 13 273 > 5 h -                      | > 5 h > 5 h > 5 h > 5 h > 5 h -                       |
| Exact BaP Cost 27 781        | 27 493 27 490 27 759 27 717 27 648 26 400 26 463 26 354 26 549 26 330                                                                                                                                   | - 14 945 15 203 14 992 15 088 14 923         | 15 030 55 480 55 383 56 143 55 717 55 802                             | - 28 546 29 109 29 042 28 641 28 403 28 748                    | - - - - - -                                           | 13 580 - 13 595 13 625 - -                            | - - - - - -                                           |
| Gap 1.42%                    | 1.07% 0.13% 3.20% 2.72% 1.71% 0.27% 0.91% 0.26% 1.17% 0.11%                                                                                                                                             | 0.54% 0.11% 0.73% 1.26% 0.46% 0.00%          | 0.51% 0.07% 0.32% 0.01% 0.14% 0.28% 0.16%                             | 1.64% 0.47% 0.36% 0.58% 0.75% 0.76%                            | 0.27% 0.52% 1.58% 0.41% 0.51% 0.66%                   | 0.01% 0.63% 0.04% 0.26% 0.35% 0.26%                   | 0.000% 0.013% 0.000% 0.000% 0.000% 0.00%              |
| Time (s) 85                  | 15 15 424 71 122 22 408 146 667 9                                                                                                                                                                       | 250 9 82 62 15 6                             | 35 159 38 543 118 1252 422                                            | 165 69 100 269 251 171                                         | 27 34 307 298 152 164                                 | 73 58 26 38 41 47                                     | 18 54 60 40 139 62                                    |
| Cost 28 176                  | 27 786 27 526 28 665 28 472 28 125 26 472 26 705 26 423 26 825 26 358                                                                                                                                   | 26 557 14 961 15 314 15 181 15 157           | 14 923 15 107 55 517 55 587 56 250 55 909 55 861 55 825               | 29 015 29 245 29 146 28 839 28 615 28 972                      | 10 964 10 998 10 963 10 989 10 984 10 908             | 13 581 13 674 13 600 13 660 13 660 13 635             | 15 735 15 736 15 727 15 703 15 683 15 717             |
| Edge-fixing %Reduc. 64% (16) | 66% (24) 66% (19) 64% (15) 67% (28) 65% (20) 55% (31) 54% (29) 55% (31) 54% (34) 53% (33)                                                                                                               | 54% (32) 58% (26) 57% (30) 58% (30) 58% (33) | 59% (36) 58% (31) 49% (18) 50% (20) 49% (20) 50% (19) 50% (25)        | 50% (20) 53% (33) 53% (40) 53% (39) 54% (42) 53% (41) 54% (39) | 55% (30) 54% (45) 53% (38) 52% (37) 52% (32) 56% (36) | 65% (54) 64% (53) 66% (56) 66% (59) 65% (59) 65% (56) | 64% (65) 66% (76) 64% (70) 65% (74) 63% (69) 65% (71) |
| Accuracy 67%                 | 76% 78% 69% 60% 70% 87% 76% 87% 68% 78% 79%                                                                                                                                                             | 79% 82% 69% 87% 82% 80%                      | 81% 79% 76% 76% 78%                                                   | 78% 64% 85% 78% 80% 71% 75%                                    | 69% 87% 65% 77% 55% 70%                               | 79% 88% 80% 75% 81% 80%                               | 86% 85% 86% 82% 87% 85%                               |
| MLmodel metrics TPR 70%      | 86% 77% 78% 74% 77% 84% 70% 85% 72% 60%                                                                                                                                                                 | 74% 66% 81% 69% 84% 64%                      | 73% 74% 74% 76% 72% 77%                                               | 75% 63% 81% 75% 79% 66% 73%                                    | 62% 88% 65% 75% 52% 68%                               | 71% 81% 77% 78% 70% 75%                               | 72% 75% 71% 71% 74% 73%                               |
| TNR 64%                      | 66% 78% 60% 45% 63% 89% 81% 88% 64% 96% 84%                                                                                                                                                             | 91% 83% 69% 89% 100%                         | 86% 88% 84% 76% 80% 79%                                               | 81% 64% 89% 81% 80% 76% 78%                                    | 75% 85% 64% 78% 57% 72%                               | 86% 95% 82% 72% 91% 85%                               | 100% 94% 100% 92% 100% 97%                            |
| sim (  o ,  m ) 83%        | 61% 80% 64% 64% 70% 60% 68% 59% 50% 88%                                                                                                                                                                 | 65% 86% 63% 72% 64% 92%                      | 75% 59% 58% 49% 57% 51%                                               | 55% 64% 60% 60% 57% 68% 62%                                    | 80% 54% 58% 56% 95% 69%                               | 89% 77% 82% 76% 91% 83%                               | 89% 87% 90% 90% 85% 88%                               |
|  m cost 27 781              | 27 493 27 490 27 775 27 717 27 651 26 400 26 463 26 354 26 515 26 330 26 412                                                                                                                            | 14 945 15 203 14 992 15 088 14 923           | 15 030 55 480 55 409 56 244 55 829 55 705                             | 55 733 28 546 29 109 29 042 28 673 28 403 28 755               | 10 935 10 941 10 792 10 944 10 928 10 908             | 13 580 13 588 13 595 13 625 13 612 13 600             | 15 735 15 734 15 727 15 703 15 683 15 716             |
|  m _10L_1                   | _10L_2 _10L_3 _10L_4 _10L_5 - _10L_1 _10L_2 _10L_3 _10L_4 _10L_5                                                                                                                                        | - _10L_1 _10L_2 _10L_3                       | _10L_4 _10L_5 - _10L_1 _10L_2 _10L_3 _10L_4 _10L_5                    | - _10L_1 _10L_2 _10L_3 _10L_4 _10L_5 -                         | _10L_1 _10L_2 _10L_3 _10L_4 _10L_5                    | - _10L_1 _10L_2 _10L_3 _10L_4 _10L_5 -                | X-n143-k07 _10L_1 _10L_2 _10L_3 _10L_4 _10L_5 -       |
|  o X-n101-k25               | Average X-n106-k14                                                                                                                                                                                      | Average X-n110-k13                           | Average X-n125-k30 Average                                            | X-n129-k18 Average                                             | X-n134-k13 Average                                    | X-n139-k10 Average                                    | Average                                               |

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

TABLE A4 Results for instances with Nc = 20 and small (S) intervals.

![Image](image_000006_286c73cd691c8df296fc7bc8580ffd6cf3b26ac026ffa9b8759eaef19841fd53.png)

| Gap                                                                                                                                                                                             | 3.19% 2.19% 2.46% 1.66% 3.85% 2.67% 2.00% 2.17% 2.04% 1.95% 1.49% 1.93% 0.76% 0.87% 1.14% 1.46% 1.50% 1.15% 5.40% 5.13% 5.54% 4.77% 5.11% 5.19% 2.74% 2.09% 2.59% 2.61% 1.73% 2.35% 3.92% 3.10% 3.90% 3.80% 4.29% 3.80% 1.54% 1.43% 1.87% 2.47% 2.12% 1.89% 2.39% 3.17% 2.72% 4.22% 4.88% 3.47%                                              |
|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| DACT Cost 28 402 28 145 28 137 27 980 28 436 28 220 26 884 26 888 26 897 26 804 26 753                                                                                                          | 26 845 15 153 15 116 15 219 15 148 15 122 15 152 58 525 58 331 58 771 58 010 58 221 58 372 29 357 30 012 30 056 29 886 29 204 29 703 11,344 11 256 11 324 11 315 11 377 11 323 13 815 13 819 13 856 13 961 13 897 13 870 16 083 16 231 16 158 16 386 16 449 16 261                                                                           |
| Ratio 10.8 12.1 8.7 6.5 4.8 8.6 37.0 13.7 11.9 13.9 366.0 88.5 17.5 26.0 212.4 11.4                                                                                                             | 10.3 55.5 - 7.9 42.0 - - - 59.6 17.6 11.0 64.6 229.8 76.5 - - - - - - - 192.4 66.4 - 87.0 - - - - - - -                                                                                                                                                                                                                                      |
| Exact BaP Time (s) 524 140 543 328 461 227 522 288 382 171 486 231 357 407 318 453 360 784 292 432 355 3660 336 1147 039 297 985 208                                                            | 047 1487 930 114 899 41 980 429 > 5 h 481 3074 11 840 > 5 h > 5 h - 1670 6230 1894 6979 12 182 5791 > 5 h > 5 h > 5 h > 5 h > 5 h - > 5 h 6925 13 074 > 5 h 11 745 - > 5 h > 5 h > 5 h > 5 h > 5 h -                                                                                                                                         |
| Gap Cost 0.86% 27 0.00% 27 0.33% 27 0.68% 27 0.70% 27 0.51% 27 0.13% 26 0.16% 26 0.62% 26 0.30% 26 0.00% 26 0.24% 26 0.19% 15 0.32% 14 0.00% 15 0.63% 14 0.32% 0.29%                            | 14 14 - 55 55 612 - - - 28 523 29 391 29 296 29 126 28 708 29 009 - - - 10 901 - - - 13 624 13 595 - 13 608 - - - - - - -                                                                                                                                                                                                                    |
| (s) 26 44 36 29 11 33                                                                                                                                                                           | 0.11% 0.11% - 0 . 11% 0.04% 0.00% 0.03% 0.35% 1.82% 1.95% 1.58% 0.94% 1.33% 0.98% 0.15% 0.28% 0.14% 0.04% 0.32% 0.00% 0.00% 0.04% 0.00% 0.08% 0.02% 0.00% 0.00% 0.00% 0.00% 1.03% 0.21%                                                                                                                                                      |
| Time 13 27 66 31 10 30 17 8                                                                                                                                                                     | 7 10 4 9 189 387 282 154 151 233 28 354 172 108 53 143 526 40 96 39 49 150 60 36 197 87 135 103 38 13 26 56 16 30                                                                                                                                                                                                                            |
| Edge-fixing %Reduc. Cost (23) 27 761 (25) 27 543 (19) 27 551 (25) 27 709 (28) 27 575 (24) 27 628 (35) 26 391 (38) 26 359 (40) 26 524 (40) 26 372 (42) 26 361 26 401 15 068 15 033 15 047 15 024 | 14 946 15 024 55 589 55 546 55 624 55 394 55 390 55 509 28 675 29 932 29 868 29 587 28 977 29 408 11 023 10 934 10 929 10 916 10 913 10 943 13 605 13 624 13 607 13 624 13 619 13 616 15 708 15 733 15 730 15 722 15 845 15 748                                                                                                              |
| Accuracy 79% 67% 73% 68% 81% 65% 65% 67% 67% 54% 53% 54% 53% 54%                                                                                                                                | 54% (39) 60% (37) 60% (40) 60% (39) 61% (40) 62% (48) 60% (41) 49% (19) 49% (24) 49% (23) 48% (28) 47% (27) 48% (24) 57% (37) 56% (45) 59% (48) 54% (42) 57% (50) 56% (44) 51% (42) 54% (39) 53% (43) 52% (46) 52% (45) 54% (43) 72% (66) 71% (70) 72% (71) 71% (74) 71% (73) 72% (71) 77% (81) 77% (87) 76% (86) 76% (82) 76% (90) 75% (85) |
| TPR 82% 70% 75% 69% 67% 78% 70% 75% 74% 68% 80% 72% 84% 64% 69% 82% 87% 69% 85% 71% 81% 77% 74% 64% 74%                                                                                         | 72% 86% 82% 84% 76% 84% 74% 80% 67% 74% 76% 81% 71% 76% 60% 74% 68% 80% 68% 77% 78% 76% 79% 76% 77% 72% 76% 76% 79% 73% 78% 74% 79% 82% 59% 70% 76% 82% 75% 84% 55% 68% 69% 77% 81% 81% 80% 90% 78% 85% 82% 91% 82% 82% 81% 86% 89% 95% 88% 94% 82% 91% 82% 91% 88% 69% 86% 88%                                                              |
| MLmodel metrics TNR 75% 75% 87% 64% 62% 73% 92% 96% 73% 91% 100% 90% 70% 83% 100%                                                                                                               | 86% 92% 86% 80% 86% 81% 87% 92% 85% 74% 72% 66% 76% 66% 71% 85% 80% 87% 92% 81% 85% 81% 100% 92% 100% 82% 91% 100% 100% 100% 100% 50% 90%                                                                                                                                                                                                    |
| sim (  o ,  m ) 73% 96% 84% 88% 73% 83% 76% 72% 74% 60% 78% 72% 63% 92% 84%                                                                                                                   | 69% 79% 77% 62% 57% 57% 74% 65% 63% 60% 55% 57% 57% 52% 56% 57% 86% 63% 65% 92% 73% 85% 89% 91% 86% 83% 87% 87% 87% 93% 93% 69% 86%                                                                                                                                                                                                          |
|  m cost 27 524 27 543 27 461 27 522 27 382 27 486 26 357 _20S_2 26 318 _20S_3 26 360 _20S_4 26 292 _20S_5 26 361 26 338 _20S_1 15 039 14 985                                                   | 15 047 14 930 14 899 14 980 55 528 55 486 55 688 55 371 55 390 55 493 28 574 29 397 29 296 29 127 28 708 29 020 10 916 10 918 10 899 10 901 10 909 10 909 13 605 13 624 13 602 13 624 13 608 13 613 15 708 15 733 15 730 15 722 15 684 15 715                                                                                                |
|  m _20S_1 _20S_2 _20S_3 _20S_4 _20S_5 - _20S_1                                                                                                                                                 | _20S_2 _20S_3 _20S_4 _20S_5 - _20S_1 _20S_2 _20S_3 _20S_4 _20S_5 - _20S_1 _20S_2 _20S_3 _20S_4 _20S_5 - _20S_1 _20S_2 _20S_3 _20S_4 _20S_5 - _20S_1 _20S_2 _20S_3 _20S_4 _20S_5 - _20S_1 _20S_2 _20S_3 _20S_4 _20S_5                                                                                                                         |
| -                                                                                                                                                                                               | Average X-n125-k30 -                                                                                                                                                                                                                                                                                                                         |
| o X-n101-k25 Average Average                                                                                                                                                                    | Average Average Average Average                                                                                                                                                                                                                                                                                                              |
|  X-n106-k14 X-n110-k13                                                                                                                                                                         | X-n129-k18 X-n134-k13 X-n139-k10 X-n143-k07 Average                                                                                                                                                                                                                                                                                          |

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## TABLE A5 Results for instances with Nc = 20 and medium (M) intervals.

![Image](image_000007_a59c6554a5ffb653f8fcf3e558dd4c883423738c5c9c8423e412548261ba71c3.png)

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

TABLE A6 Results for instances with Nc = 20 and large (L) intervals.

![Image](image_000008_f88ea7afcf428f83986c2437419417cd8bb1d0f0397795f6aa311b2b5d37cfba.png)

|                                                                                                                                                                                                                  | Gap 2.76% 2.37% 2.09% 1.58% 2.04% 2.17% 1.56% 1.15% 1.70% 1.95% 1.61% 1.59% 1.49% 1.05% 1.52% 1.10% 0.91% 1.21% 4.32% 5.20% 5.62% 4.07% 4.67% 4.77% 3.47% 2.48% 1.47% 3.16% 1.89% 2.49% 3.99% 3.85% 3.13% 2.84% 4.92% 3.75% 1.58% 1.49% 1.41% 1.52% 1.47% 1.50% 4.25% 3.11% 3.90% 3.52%                                | Gap 2.76% 2.37% 2.09% 1.58% 2.04% 2.17% 1.56% 1.15% 1.70% 1.95% 1.61% 1.59% 1.49% 1.05% 1.52% 1.10% 0.91% 1.21% 4.32% 5.20% 5.62% 4.07% 4.67% 4.77% 3.47% 2.48% 1.47% 3.16% 1.89% 2.49% 3.99% 3.85% 3.13% 2.84% 4.92% 3.75% 1.58% 1.49% 1.41% 1.52% 1.47% 1.50% 4.25% 3.11% 3.90% 3.52%                                |
|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| DACT Cost 28 221 28 158 28 369 28 276                                                                                                                                                                            | 28 194 28 244 26 759 26 907 26 853 26 871 26 782 26 834 15 234 15 160 15 291 15 193 15 203 15 216 57 981 59 339 59 042 58 746 58 035 58 629 30 263 30 198 30 198 29 405 29 177 29 848 11 370 11 343 11 331 11 276 11 264 11 317 13 798 13 785 13 771 13 771 13 814 13 778 16 471 16 193 16 432 16 373 16 190 16 332    | 28 194 28 244 26 759 26 907 26 853 26 871 26 782 26 834 15 234 15 160 15 291 15 193 15 203 15 216 57 981 59 339 59 042 58 746 58 035 58 629 30 263 30 198 30 198 29 405 29 177 29 848 11 370 11 343 11 331 11 276 11 264 11 317 13 798 13 785 13 771 13 771 13 814 13 778 16 471 16 193 16 432 16 373 16 190 16 332    |
| Ratio 3.5 2.1 2.0 101.0 45.5 30.8 - 71.2                                                                                                                                                                         | 60.3 2.0 - - 15.4 42.0 17.1 7.2 44.7 25.4 - 6.4 - - - - 5.1 4.1 20.1 46.6 4.0 16.0 - - - - 134.8 - 104.2 120.8 169.5 25.8 - - - - - - - -                                                                                                                                                                              | 60.3 2.0 - - 15.4 42.0 17.1 7.2 44.7 25.4 - 6.4 - - - - 5.1 4.1 20.1 46.6 4.0 16.0 - - - - 134.8 - 104.2 120.8 169.5 25.8 - - - - - - - -                                                                                                                                                                              |
| Time (s) 374 196 100 707 955 466                                                                                                                                                                                 | > 5 h 12 168 5847 401 > 5 h - 231 386 307 351 1162 487 > 5 h 2428 > 5 h > 5 h > 5 h - 1455 1557 6152 2751 3614 3106 > 5 h > 5 h > 5 h > 5 h 12 945 - 16 053 4350 5595 10 501 > 5 h - > 5 h > 5 h > 5 h > 5 h > 5 h -                                                                                                   | > 5 h 12 168 5847 401 > 5 h - 231 386 307 351 1162 487 > 5 h 2428 > 5 h > 5 h > 5 h - 1455 1557 6152 2751 3614 3106 > 5 h > 5 h > 5 h > 5 h 12 945 - 16 053 4350 5595 10 501 > 5 h - > 5 h > 5 h > 5 h > 5 h > 5 h -                                                                                                   |
| Exact BaP Cost 27 463 27 507 27 789 27 837 27 631 27 645 - 26 593 26 387 26 358 - - 15 011 15 003 15 062 15 027 15 066 15 034 - 56 407 -                                                                         | - - - 29 149 - - - - 10 736 - 13 583 13 580 13 579 13 565 13 638 - - - - - -                                                                                                                                                                                                                                           | 29 468 29 709 28 501                                                                                                                                                                                                                                                                                                   |
| Gap 0.63% 0.49% 1.64% 0.93% 0.27% 0.73% 0.95% 0.61% 0.13% 0.54% 0.16% 1.49% 1.33% 0.82% 0.43% 0.84% 0.43% 0.82% - 0 . 01%                                                                                        | 0.41% 0.22% 0.37% 0.61% 0.10% 0.22% 0.13% 0.72% 0.10% 0.25% 0.09% 0.06% 0.45% 0.40% 0.00% 0.20%                                                                                                                                                                                                                        | 1.83% 1.37% 0.92%                                                                                                                                                                                                                                                                                                      |
| Time (s) 107 0.37% 0.81% 171 199 100 997 379 181                                                                                                                                                                 | 508 100 433 286 96 254 154 36 33 407 64 139 23 76 19 74 13 41                                                                                                                                                                                                                                                          | 380 306                                                                                                                                                                                                                                                                                                                |
| Cost 27 637 27 641 28 245 28 097 27 733 27 871 26 420 26 792 26 655 26 520 26 390 26 555 15 035 15 226 15 262 15 150 15 131 15 161 55 824 56 870 55 898                                                          | 56 680 55 567 56 168 29 428 11 051 13 597 13 612 13 597 13 662 13 628 13 619 15 814 15 715 15 886 15 879 15 580 15 775                                                                                                                                                                                                 | 30 008 30 170 28 766                                                                                                                                                                                                                                                                                                   |
| Edge-fixing %Reduc. 52% (14) 57% (21) 55% (20) 57% (22) 55% (22) 55% (20) 55% (32) 53% (33) 53% (40) 53% (41) 54% (38) 53% (37) 59% (32) 59% (36) 58% (29) 61% (40) 59% (39) 60% (35) 51% (22) 49% (16) 50% (25) | 48% (18) 50% (23) 49% (21) 49% (31) 52% (40) 51% (42) 52% (40) 59% (55) 61% (63) 58% (57) 60% (59) 60% (63) 59% (59) 70% (79) 70% (84) 72% (89) 70% (86) 70% (84) 70% (84)                                                                                                                                             | 49% (30) 51% (34) 53% (38)                                                                                                                                                                                                                                                                                             |
| Accuracy 67% 70% 81% 71% 72% 72% 82% 73% 80% 78% 73% 77% 72% 80% 81% 80% 87% 80% 71% 81% 76%                                                                                                                     | 70% 75% 74% 78% 72% 81% 78% 83% 86% 79% 89% 83% 85% 76% 67% 80% 90% 79%                                                                                                                                                                                                                                                | 69% 82% 73%                                                                                                                                                                                                                                                                                                            |
| metrics TPR 63% 70% 85% 76% 73% 73% 76% 73% 74% 77% 71% 74% 69% 73% 83% 83% 85%                                                                                                                                  | 79% 69% 76% 68% 63% 73% 70% 74% 63% 81% 76% 74% 74% 83% 85% 82% 80% 70% 80% 68% 79% 72% 81% 87% 77% 78% 76% 82% 81% 79% 79%                                                                                                                                                                                            | 79% 69% 76% 68% 63% 73% 70% 74% 63% 81% 76% 74% 74% 83% 85% 82% 80% 70% 80% 68% 79% 72% 81% 87% 77% 78% 76% 82% 81% 79% 79%                                                                                                                                                                                            |
| MLmodel TNR 70% 69% 76% 65% 70% 70% 87% 72% 85% 79% 74% 79% 75% 86% 78% 76% 88% 81% 73% 85% 83%                                                                                                                  | 76% 76% 79% 81% 75% 82% 70% 78% 77% 86% 86% 85% 81% 74% 82% 87% 87% 100% 77% 90% 88% 91% 75% 52% 78% 100% 79%                                                                                                                                                                                                          | 76% 76% 79% 81% 75% 82% 70% 78% 77% 86% 86% 85% 81% 74% 82% 87% 87% 100% 77% 90% 88% 91% 75% 52% 78% 100% 79%                                                                                                                                                                                                          |
| sim (  o ,  m ) 67% 66% 51% 53% 59% 59% 66% 55% 65% 57% 62% 61% 77% 76% 59% 63% 65% 68% 57% 56% 65%                                                                                                            | 61% 53% 58% 54% 54% 54% 56% 55% 84% 73% 81% 63% 65% 73% 88% 89% 71% 82% 89% 84%                                                                                                                                                                                                                                        | 64% 52% 50%                                                                                                                                                                                                                                                                                                            |
|  m cost 27 463 27 507 27 789 27 837 27 631 27 645 26 348 26 599 26 405 26 358 26 357 26 413 15 011 15 003 15 062                                                                                                | 29 123 10 934 10 922 10 987 10 965 10 736 10 909 13 583 13 582 13 579 13 565 13 614 13 585 15 799 15 705 15 815 15 816                                                                                                                                                                                                 | 29 123 10 934 10 922 10 987 10 965 10 736 10 909 13 583 13 582 13 579 13 565 13 614 13 585 15 799 15 705 15 815 15 816                                                                                                                                                                                                 |
|  m _20L_1 _20L_2 _20L_3 _20L_4 _20L_5 - _20L_1 _20L_2 _20L_3 _20L_4 _20L_5 - _20L_1 _20L_2 _20L_3                                                                                                               | _20L_4 15 027 _20L_5 15 066 - 15 034 _20L_1 55 584 _20L_2 56 407 _20L_3 55 903 _20L_4 56 451 _20L_5 55 445 - 55 958 _20L_1 29 249 _20L_2 29 468 _20L_3 29 761 _20L_4 28 504 _20L_5 28 635 - _20L_1 _20L_2 _20L_3 _20L_4 _20L_5 - _20L_1 _20L_2 _20L_3 _20L_4 _20L_5 - _20L_1 _20L_2 _20L_3 _20L_4 _20L_5 15 580 15 743 | _20L_4 15 027 _20L_5 15 066 - 15 034 _20L_1 55 584 _20L_2 56 407 _20L_3 55 903 _20L_4 56 451 _20L_5 55 445 - 55 958 _20L_1 29 249 _20L_2 29 468 _20L_3 29 761 _20L_4 28 504 _20L_5 28 635 - _20L_1 _20L_2 _20L_3 _20L_4 _20L_5 - _20L_1 _20L_2 _20L_3 _20L_4 _20L_5 - _20L_1 _20L_2 _20L_3 _20L_4 _20L_5 15 580 15 743 |
| o                                                                                                                                                                                                                | Average X-n129-k18 Average X-n134-k13 Average X-n139-k10 Average X-n143-k07                                                                                                                                                                                                                                            | Average X-n129-k18 Average X-n134-k13 Average X-n139-k10 Average X-n143-k07                                                                                                                                                                                                                                            |
|  X-n101-k25                                                                                                                                                                                                     | Average                                                                                                                                                                                                                                                                                                                | Average                                                                                                                                                                                                                                                                                                                |
| Average X-n106-k14                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                        |
|                                                                                                                                                                                                                  | -                                                                                                                                                                                                                                                                                                                      | -                                                                                                                                                                                                                                                                                                                      |
| Average                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                        |
| X-n110-k13 Average X-n125-k30                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                        |                                                                                                                                                                                                                                                                                                                        |

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

TABLE A7 Results for instances with Nc = 30 and small (S) intervals.

![Image](image_000009_b100351f858c093f820d3dd7eed94d4d3da1eddc38c3c40c36889542a3b44ec4.png)

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

## TABLE A8 Results for instances with Nc = 30 and medium (M) intervals.

![Image](image_000010_b12821d0f69c3c69d390f6fdea355e19a7189fb1325f9cb116d5d17e1b28d796.png)

| Gap                 | 0.97% 2.45% 2.53% 2.40% 2.92% 2.25% 1.79% 0.96% 1.39% 0.66% 1.69% 1.30% 1.71% 2.83% 1.43% 0.30% 1.48% 1.55% 6.02% 5.67% 5.18% 5.09% 4.27% 5.25% 2.55% 1.67% 1.04% 3.30% 2.23% 2.16% 1.91% 3.49% 4.24% 4.43% 3.11% 3.43% 2.16% 3.19% 1.74% 2.28% 1.11% 2.09% 3.43% 4.38% 7.05% 2.01% 3.42%   |                                                       |                                                       |                                                                |                                                  |                                                       |                                                                | 4.06%                                        |
|---------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------|--------------------------------------------------|-------------------------------------------------------|----------------------------------------------------------------|----------------------------------------------|
| DACT Cost           | 28 118 28 338 28 401 28 388 28 139                                                                                                                                                                                                                                                          | 28 277 26 892 26 804 26 805 26 621 26 779             | 26 780 15 443 15 502 15 346 15 040 15 130             | 15 292 58 917 59 406 59 519 58 574 57 880                      | 58 859 30 360 30 256 30 122 29 629 28 878 29 849 | 11 168 11 360 11 464 11 427 11 086 11 301             | 13 926 14 235 13 834 13 927 13 710 13 926 16 292               | 16 741 16 912 16 005 16 165 16 423           |
| Ratio               | 2.2 1.4 11.1 2.2 107.0 24.8                                                                                                                                                                                                                                                                 | 7.9 - - - -                                           | - 42.4 19.7 21.4 7.1                                  | 77.4 33.6 1.6 11.2 5.5 60.8 - -                                | 17.3 36.8 52.7 3.3 63.9 34.8 -                   | - - - - -                                             | 62.0 - 189.3 23.4 90.9 - -                                     | - - - - -                                    |
| Time (s)            | 244 91 89 149 642 243                                                                                                                                                                                                                                                                       | 599 > 5 h > 5 h > 5 h > 5 h -                         | 339 177 150 291 387 269                               | 1425 1280 3800 5232 > 5 h - 5594                               | 3348 4689 406 3835 3574 > 5 h                    | > 5 h > 5 h > 5 h > 5 h -                             | 8248 > 5 h 10 792 7536 16 734 - > 5 h                          | > 5 h > 5 h > 5 h > 5 h -                    |
| Exact BaP Cost      | 27 830 27 660 27 699 27 724 27 340 27 651                                                                                                                                                                                                                                                   | 26 419 26 579 26 656 - - -                            | 15 183 15 075 15 129 14 995 14 910                    | 15 058 55 574 56 128 56 314 55 656 - -                         | 29 604 29 758 29 793 28 669 28 249 29 215 -      | - - - - -                                             | 13 631 - 13 598 13 617 13 560 -                                | - - - - - -                                  |
| Gap                 | 0.97% 1.10% 0.07% 0.43% 0.13% 0.54%                                                                                                                                                                                                                                                         | 0.41% 0.72% 0.84% 1.08% 0.33% 0.67%                   | 0.30% 0.03% 0.22% 0.65% 0.00%                         | 0.24% 0.25% 0.13% 0.18% 0.10% 0.09% 0.15%                      | 1.69% 2.18% 1.37% 1.85% 1.05% 1.63% 0.73%        | 1.28% 1.28% 0.85% 1.44% 1.12%                         | 0.81% 0.99% 0.00% 0.08% 0.58% 0.49% 0.00%                      | 1.91% 0.45% 0.00% 0.40% 0.55%                |
| Time (s)            | 110 66 8 67 6 51                                                                                                                                                                                                                                                                            | 76 312 73 769 7 247                                   | 8 9 7 41 5 14                                         | 892 114 685 86 476 451 324                                     | 91 89 122 60 137                                 | 36 108 372 70 60 129                                  | 133 585 57 322 184 256 7                                       | 199 16 7 15 49                               |
| Cost                | 28 118 27 964 27 718 27 843 27 375 27 804                                                                                                                                                                                                                                                   | 26 526 26 741 26 659 26 731 26 419 26 615             | 15 228 15 079 15 163 15 093 14 910 15 095             | 55 711 56 292 56 691 55 793 55 558 56 009 30 105               | 30 408 30 221 29 212 28 545 29 698               | 11 039 11 118 11 139 11 035 10 907 11 048             | 13 742 13 932 13 598 13 628 13 638 13 708 15 751               | 16 346 15 869 15 690 15 692 15 870           |
| Edge-fixing %Reduc. | 57% (17) 56% (18) 59% (19) 56% (21) 61% (27) 58% (20)                                                                                                                                                                                                                                       | 50% (35) 53% (38) 54% (41) 51% (38) 52% (40) 52% (38) | 58% (33) 58% (33) 59% (41) 58% (36) 59% (38) 58% (36) | 50% (22) 47% (19) 49% (21) 46% (23) 48% (27) 48% (22) 48% (34) | 52% (46) 48% (35) 50% (42) 52% (41) 50% (40)     | 53% (32) 52% (40) 52% (42) 51% (35) 54% (39) 52% (38) | 63% (60) 61% (54) 59% (52) 61% (57) 61% (56) 61% (56) 74% (80) | 73% (81) 73% (84) 73% (83) 74% (82) 72% (82) |
| Accuracy            | 71% 70% 71% 69% 73% 71%                                                                                                                                                                                                                                                                     | 63% 69% 78% 72% 83% 73%                               | 68% 73% 82% 86% 88% 79%                               | 79% 84% 78% 78% 82% 80% 75%                                    | 81% 76% 80% 75% 77%                              | 84% 81% 80% 85% 78% 81%                               | 84% 79% 82% 89% 85% 83% 93%                                    | 76% 82% 82% 76% 82%                          |
| MLmodel metrics TPR | 75% 72% 72% 71% 76% 73%                                                                                                                                                                                                                                                                     | 60% 71% 74% 70% 76% 70%                               | 65% 66% 78% 75% 76% 72%                               | 77% 84% 76% 76% 78% 78% 67%                                    | 82% 72% 77% 77% 75% 85%                          | 81% 73% 80% 81% 80%                                   | 76% 76% 68% 87% 82% 78% 86%                                    | 89% 83% 79% 90% 85%                          |
| TNR                 | 66% 67% 69% 67% 70% 68%                                                                                                                                                                                                                                                                     | 66% 66% 81% 73% 89% 75%                               | 71% 80% 86% 96% 100% 87%                              | 80% 84% 80% 79% 85% 82% 82%                                    | 80% 80% 83% 72% 79% 83%                          | 80% 86% 90% 75% 83%                                   | 91% 82% 95% 90% 87% 89% 100%                                   | 63% 80% 84% 62% 78%                          |
| sim (  o ,  m )   | 57% 60% 69% 60% 68% 63%                                                                                                                                                                                                                                                                     | 61% 52% 63% 56% 63% 59%                               | 81% 83% 70% 76% 77% 77%                               | 52% 45% 52% 46% 53% 50% 61%                                    | 51% 54% 55% 48% 54%                              | 53% 52% 65% 58% 52% 56%                               | 80% 74% 86% 66% 70% 75%                                        | 86% 69% 84% 91% 70% 80%                      |
|  m cost            | 27 849 27 660 27 699 27 724 27 340 27 654                                                                                                                                                                                                                                                   | 26 419 26 550 26 437 26 446 26 333 26 437             | 15 183 15 075 15 129 14 995 14 910 15 058             | 55 574 56 218 56 589 55 735 55 510 55 925 29 604               | 29 758 29 813 28 682 28 249 29 221 10 959        | 10 977 10 998 10 942 10 752 10 926                    | 13 631 13 795 13 598 13 617 13 560 13 640 15 751               | 16 039 15 798 15 690 15 630 15 782           |
|  m                 | _30M_1 _30M_2 _30M_3 _30M_4 _30M_5                                                                                                                                                                                                                                                          | - _30M_1 _30M_2 _30M_3 _30M_4 _30M_5                  | - _30M_1 _30M_2 _30M_3 _30M_4 _30M_5                  | - _30M_1 _30M_2 _30M_3 _30M_4 _30M_5 - _30M_1                  | _30M_2 _30M_3 _30M_4 _30M_5 - _30M_1             | _30M_2 _30M_3 _30M_4 _30M_5 -                         | _30M_1 _30M_2 _30M_3 _30M_4 _30M_5 - _30M_1                    | _30M_2 _30M_3 _30M_4 _30M_5 -                |
| o                   |                                                                                                                                                                                                                                                                                             |                                                       |                                                       | X-n125-k30 Average X-n129-k18                                  | X-n134-k13                                       |                                                       |                                                                | Average                                      |
|                    | X-n101-k25                                                                                                                                                                                                                                                                                  | Average X-n106-k14                                    | Average X-n110-k13                                    | Average                                                        | Average                                          | Average X-n139-k10                                    | Average X-n143-k07                                             |                                              |

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License

TABLE A9 Results for instances with Nc = 30 and large (L) intervals.

![Image](image_000011_012ea6b4211b970ca5a92ecef4c9db4cc47dd35b46bed4c4486992b0701f5c02.png)

| Gap                     | 2.70% 2.11% 2.26% 1.97% 1.62% 2.13% 1.33%                                    | 1.53% 2.11% 1.87% 2.13% 1.79% 2.39%                                          | 1.00% 1.39% 1.19% 1.55% 1.51% 3.63%                                          | 4.91% 4.46% 5.17% 4.03% 4.44% 3.17% 3.28% 2.54%                              | 3.13% 2.71% 2.97% 5.35%                                                      | 4.77% 3.66% 3.33% 4.45% 4.31%                         | 2.14% 2.82% 1.55% 0.66% 2.08% 1.85% 3.12% 2.26%                                     | 3.20% 2.77% 2.57% 2.79%                                                      |
|-------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|------------------------------------------------------------------------------|-------------------------------------------------------|-------------------------------------------------------------------------------------|------------------------------------------------------------------------------|
| DACT Cost               | 28 486 28 045 28 438 28 358 27 709                                           | 28 207 26 976 26 907 26 901 26 932 26 851 26 913                             | 15 634 15 297 15 286 15 123 15 248 15 318                                    | 58 481 59 140 58 901 59 352 57 751 58 725 30 941                             | 31 252 30 769 30 275 29 535 30 554                                           | 11 627 11 498 11 346 11 231 11 412 11 423             | 13 854 14 082 13 859 13 669 13 876 13 868                                           | 16 566 16 474 16 364 16 148 16 079 16 326                                    |
| Ratio                   | 11.8 23.1 0.1 29.0 33.1 19.4                                                 | - 10.9 8.2 - 119.8 -                                                         | 61.2 4.4 4.3 19.0 4.5 18.7                                                   | - - 7.7 - 45.2 -                                                             | 30.3 43.0 14.1 242.7 19.5 69.9                                               | - - - 123.5 - -                                       | - 67.6 60.9 96.9 - -                                                                | - - - - - -                                                                  |
| Time (s)                | 555 300 126 348 364 339                                                      | > 5 h 6491 74 > 5 h 2157 - 795                                               | 635 252 3822 94 1120 >                                                       | 5 h > 5 h 2397 > 5 h 15 245 -                                                | 4123 3099 1201 15 778 370 4914                                               | > 5 h > 5 h > 5 h 9388 > 5 h -                        | > 5 h 9395 10 780 9597 > 5 h -                                                      | > 5 h > 5 h > 5 h > 5 h > 5 h -                                              |
| Exact BaP Cost          | 27 736 27 465 27 809 27 809 27 267 27 617                                    | - 26 494 26 345 26 438 26 268 - 15 269                                       | 15 145 15 076 14 945 15 015 15 090 -                                         | - 56 072 - 55 453 -                                                          | 29 985 30 260 30 007 29 341 28 757 29 670                                    | - - 10 950 10 868 - -                                 | - 13 696 13 648 13 580 - -                                                          | - - - - - -                                                                  |
| Gap                     | 2.03% 0.46% 1.70% 1.25% 0.20% 1.12%                                          | 0.94% 0.99% 0.32% 1.03% 0.44% 0.75%                                          | 0.51% 1.44% 0.57% 0.70% 0.79% 0.80%                                          | 0.05% 0.09% - 0 . 31% 0.11% 0.01% - 0 . 01 %                                 | 2.45% 1.57% 2.16% 2.00% 0.27% 1.69%                                          | 0.80% 0.65% 0.90% 0.65% 0.62% 0.72%                   | 0.14% 0.18% 0.14% 0.14% 0.08% 0.14%                                                 | 0.45% 1.01% 0.14% 0.00% 0.18% 0.36%                                          |
| Time (s)                | 47 13 844 12 11 185                                                          | 666 594 9 90 18 275                                                          | 13 144 59 201 21 88                                                          | 622 401 312 702 337 475                                                      | 136 72 85 65 19 75                                                           | 337 111 39 76 161 145                                 | 95 139 177 99 132 128                                                               | 1297 181 55 21 11 313                                                        |
| Cost                    | 28 298 27 590 28 281 28 156 27 321 27 929                                    | 26 874 26 765 26 430 26 710 26 406 26 637                                    | 15 347 15 363 15 162 15 050 15 134 15 211                                    | 56 463 56 422 56 210 56 494 55 517 56 221                                    | 30 723 30 736 30 656 29 944 28 836 30 179                                    | 11 125 11 045 11 043 10 940 10 994 11 029             | 13 583 13 720 13 667 13 599 13 604 13 635                                           | 16 137 16 273 15 878 15 712 15 704 15 941                                    |
| Edge-fixing %Reduc.     | 55% (18) 56% (19) 56% (20) 55% (19) 54% (21) 56% (19)                        | 46% (29) 45% (37) 46% (36) 45% (38) 47% (39) 46% (36)                        | 53% (33) 55% (40) 52% (37) 53% (39) 52% (34) 54% (37)                        | 45% (16) 45% (19) 45% (26) 44% (21) 43% (27) 45% (22)                        | 51% (35) 53% (37) 52% (40) 54% (44) 55% (46) 53% (40)                        | 52% (34) 51% (36) 51% (30) 52% (36) 53% (42) 51% (36) | 59% (56) 58% (49) 59% (60) 58% (56) 59% (61) 58% (56)                               | 69% (71) 71% (87) 70% (83) 70% (86) 71% (79) 69% (81)                        |
| Accuracy                | 76% 78% 70% 70% 78% 74%                                                      | 74% 70% 74% 80% 71% 74%                                                      | 71% 83% 80% 77% 75% 77%                                                      | 79% 74% 79% 80% 77% 78%                                                      | 75% 73% 74% 73% 78% 74%                                                      | 80% 88% 81% 79% 85% 82%                               | 83% 85% 85% 81% 86% 84%                                                             | 86% 75% 85% 89% 80% 83%                                                      |
| MLmodel metrics TNR TPR | 75% 71% 70% 69% 73% 72%                                                      | 71% 62% 66% 79% 63% 68%                                                      | 67% 85% 71% 73% 63% 72%                                                      | 74% 65% 70% 73% 72% 71%                                                      | 75% 71% 75% 74% 71% 73%                                                      | 81% 87% 77% 80% 85% 82%                               | 73% 77% 75% 82% 75% 76%                                                             | 85% 86% 80% 77% 89% 83%                                                      |
| ,  m )                 | 76% 84% 70% 70% 82% 76%                                                      | 76% 78% 82% 81% 78% 79%                                                      | 74% 81% 89% 80% 86% 82%                                                      | 83% 82% 88% 86% 82% 84%                                                      | 74% 75% 73% 71% 84% 75%                                                      | 79% 88% 84% 78% 84% 83%                               | 93% 92% 94% 79% 96% 91%                                                             | 87% 64% 90% 100% 70% 82%                                                     |
| sim (  o               | 61% 72% 66% 65% 65% 66%                                                      | 47% 58% 59% 44% 61% 54%                                                      | 66% 54% 69% 63% 78% 66%                                                      | 49% 58% 57% 50% 46% 52%                                                      | 52% 60% 53% 55% 71% 58%                                                      | 52% 52% 58% 51% 53% 53%                               | 79% 72% 77% 61% 78% 73%                                                             | 78% 70% 85% 91% 69% 79%                                                      |
|  m cost                | _30L_1 27 736 _30L_2 27 465 _30L_3 27 809 _30L_4 27 809 _30L_5 27 267 27 617 | _30L_1 26 623 _30L_2 26 502 _30L_3 26 345 _30L_4 26 438 _30L_5 26 290 26 440 | _30L_1 15 269 _30L_2 15 145 _30L_3 15 076 _30L_4 14 945 _30L_5 15 015 15 090 | _30L_1 56 435 _30L_2 56 372 _30L_3 56 386 _30L_4 56 432 _30L_5 55 514 56 228 | _30L_1 29 989 _30L_2 30 260 _30L_3 30 007 _30L_4 29 356 _30L_5 28 757 29 674 | 11 037 10 974 10 945 10 869 _30L_5 10 926             | 10 950 _30L_1 13 564 _30L_2 13 696 _30L_3 13 648 _30L_4 13 580 _30L_5 13 593 13 616 | _30L_1 16 065 _30L_2 16 110 _30L_3 15 856 _30L_4 15 712 _30L_5 15 676 15 884 |
|                        | X-n101-k25 -                                                                 | Average -                                                                    | X-n110-k13 -                                                                 | X-n125-k30 -                                                                 | Average                                                                      | - _30L_1 _30L_2 _30L_3 _30L_4                         | Average - -                                                                         | Average -                                                                    |
|  o                     | Average                                                                      | X-n106-k14                                                                   | Average                                                                      | Average                                                                      | X-n129-k18                                                                   | X-n134-k13                                            | X-n139-k10 Average                                                                  | X-n143-k07                                                                   |

10970037, 2024, 3, Downloaded from https://onlinelibrary.wiley.com/doi/10.1002/net.22200 by Technische UniversitGLYPH&lt;228&gt;t München, Wiley Online Library on [30/03/2025]. See the Terms and Conditions (https://onlinelibrary.wiley.com/terms-and-conditions) on Wiley Online Library for rules of use; OA articles are governed by the applicable Creative Commons License