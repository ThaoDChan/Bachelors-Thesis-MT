# Fair collaborative vehicle routing: A deep multi-agent reinforcement learning approach

## Metadata
- **Link to PDF**: [[[mak2023]_air_collaborative_vehicle_routing-A_deep_multi-agent_reinforcement_learning_approach.pdf]] 
- **Tags**:  
  #ML-Supervised  
  #ML-ReinforcementLearning  
  #RL-PolicyGradient  
  #RL-ActorCritic  
  #ProximalPolicyOptimization  
  #MDP  
  #MultiAgentRL  
  #TransferLearning  
  #DeterministicVRP  
  #MultiVehicleRouting  
  #GameTheory  

- **Relevant**: true  
  *Reasoning*: The paper focuses on a deterministic multi-agent VRP setting (collaborative vehicle routing), employing deep reinforcement learning. This aligns with the thesis scope on ML-based methods for deterministic VRP.  
- **Fit Score**: 9 (Highly relevant for ML + deterministic VRP)  
- **State of the Art (SoA) Concepts**:  
  - Multi-Agent Reinforcement Learning (MARL) for vehicle routing  
  - Proximal Policy Optimization (PPO) in a bargaining-game framework  
  - Fair gain sharing (Shapley value correlation)  
  - Multi-depot VRP formulation (uncapacitated variant)  
  - Coalition formation and game theory in routing  
- **Performance Evaluation**: yes  
- **Performance Evaluation Framework**: Uses custom-generated benchmark instances (not a standard library like Solomon or Christofides).  

## Abstract
“Collaborative vehicle routing occurs when carriers collaborate through sharing their transportation requests and performing transportation requests on behalf of each other. This achieves economies of scale, thus reducing cost, greenhouse gas emissions and road congestion. But which carrier should partner with whom, and how much should each carrier be compensated? Traditional game theoretic solution concepts are expensive to calculate as the characteristic function scales exponentially with the number of agents. This would require solving the vehicle routing problem (NP-hard) an exponential number of times. We therefore propose to model this problem as a coalitional bargaining game solved using deep multi-agent reinforcement learning, where - crucially - agents are not given access to the characteristic function. Instead, we implicitly reason about the characteristic function; thus, when deployed in production, we only need to evaluate the expensive post-collaboration vehicle routing problem once. Our contribution is that we are the first to consider both the route allocation problem and gain sharing problem simultaneously - without access to the expensive characteristic function. Through decentralised machine learning, our agents bargain with each other and agree to outcomes that correlate well with the Shapley value - a fair profit allocation mechanism. Importantly, we are able to achieve a reduction in run-time of 88%.”

## Summary
- **Problem & Motivation**  
  - Addresses the challenge of *collaborative vehicle routing* (CVR), where multiple carriers collaborate to share deliveries.  
  - Goals: (1) Determine *which carriers* should form coalitions, (2) How to *fairly* allocate the resulting cost savings (or gains) among carriers.  
  - Traditional game theory (computing solution concepts like the Shapley value) requires solving an exponentially large set of NP-hard vehicle routing instances (the characteristic function), making it computationally intractable for more than a handful of carriers.

- **Approach**  
  - Proposes a *coalitional bargaining game* framework grounded in game theory, combined with a *deep multi-agent reinforcement learning (MARL)* method.  
  - Each carrier is modeled as an *autonomous agent* that negotiates over which coalition to form and how to split the collaboration gain.  
  - The MARL model uses *Proximal Policy Optimization (PPO)* with an *actor-critic* scheme.  
  - Agents are trained to propose and respond to coalition offers (who joins the coalition, how gains are allocated).  
  - *Pre-training* step: A supervised module first approximates the collaboration gain from the carriers’ location data, providing a high-quality feature extractor. This accelerates RL convergence.  
  - *Decentralization*: Agents do not have to know the full characteristic function; instead, they learn to *implicitly* approximate it from the graph-based delivery data.  

- **Key Contributions**  
  1. Demonstrates that *simultaneous solving* of route allocation and gain sharing is feasible without enumerating the entire characteristic function.  
  2. *Deep MARL* approach recovers outcomes correlating highly with Shapley values (a widely accepted fair allocation method) but avoids exponential VRP enumeration.  
  3. *Significant runtime savings*: Only one large VRP solve is necessary (the coalition that forms), yielding an 88% reduction in computational effort relative to standard game theoretic solutions.  

- **Method & Experiments**  
  - Uses a *three-agent* scenario, each with one depot and three customers, culminating in a small multi-depot VRP (effectively uncapacitated).  
  - Pre- and post-collaboration routes are computed exactly with Gurobi, though only once in the final stage.  
  - Over multiple training epochs, agents learn to form coalitions efficiently (often excluding low-value partners) and propose fair cost-splitting based on *PPO* with discount factors.  
  - The authors perform thorough *ablation studies* (varying the number of bargaining rounds, discount factors) to illustrate stability and performance.  
  - They measure *optimality gap* (comparing proposed coalitions to the best possible coalition) and the *allocation fairness* (comparing to Shapley values). Results show ~4% suboptimality on average, with strong correlation to Shapley allocations.

- **Strengths**  
  - Integrates *MARL* with *coalitional bargaining* in a fully *deterministic* VRP setting.  
  - Achieves near-Shapley fair allocations without enumerating all possible coalitions.  
  - Efficient run time compared to classical approach that requires full characteristic function.  
  - Potentially scalable to more complex constraints or extended VRP variants (time windows, etc.), since the approach mainly modifies how agents bargain.

- **Limitations & Future Work**  
  - Current experiments focus on a *small scale* (three carriers, each with one vehicle). Scaling to many carriers or large VRPs may require further algorithmic innovations.  
  - Perfect information assumption: real-world collaboration might involve partial information or strategic misreporting. This could require *decentralized partially observable MDP* approaches.  
  - Capacity, time windows, or heterogeneous fleet constraints are left as future extensions; the authors argue their approach is flexible enough to incorporate these.  
  - Potential to integrate more advanced ML or game-theoretic concepts for large-scale collaboration.

- **Relevance to Deterministic VRP & Thesis**  
  - Provides a novel *hybrid game-theoretic + MARL* approach directly applicable to deterministic VRP collaboration scenarios.  
  - Illustrates that cost-sharing negotiations can be automated via RL and still yield fair, near-optimal outcomes.  
  - Highlights *pre-training* to facilitate function approximation, an important technique for large-scale or complex deterministic VRPs.  
  - Underscores the importance of bridging classical game theory with modern RL to tackle multi-carrier routing cooperation.  
